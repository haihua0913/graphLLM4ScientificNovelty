node1,node2,similarity,distance
probability of winning,zero-sum games,0.9328,0.3665
cumulative intermediate reward,zero-sum games,0.9383,0.3512
cumulative intermediate reward,probability of winning,0.9804,0.1979
thresholded rewards,zero-sum games,0.9596,0.2841
cumulative intermediate reward,thresholded rewards,0.901,0.4451
Markov Decision Processes,thresholded rewards,0.6168,0.8755
finite-horizon,thresholded rewards,0.9488,0.3199
Markov Decision Processes,zero-sum games,0.5418,0.9573
finite-horizon,zero-sum games,0.8421,0.562
Markov Decision Processes,cumulative intermediate reward,0.4037,1.092
cumulative intermediate reward,finite-horizon,0.7705,0.6774
Markov Decision Processes,finite-horizon,0.7485,0.7093
optimal policy,thresholded rewards,0.1157,1.3299
non-stationary,thresholded rewards,0.4615,1.0378
optimal policy,zero-sum games,0.0873,1.3511
non-stationary,zero-sum games,0.449,1.0498
cumulative intermediate reward,optimal policy,-0.0652,1.4596
cumulative intermediate reward,non-stationary,0.2417,1.2315
Markov Decision Processes,optimal policy,0.1508,1.3032
Markov Decision Processes,non-stationary,0.9362,0.3573
finite-horizon,optimal policy,0.0352,1.3891
finite-horizon,non-stationary,0.5634,0.9344
non-stationary,optimal policy,0.2066,1.2597
thresholded rewards,value iteration algorithm,-0.3665,1.6532
cumulative intermediate reward,value iteration algorithm,-0.1339,1.5059
Markov Decision Processes,value iteration algorithm,-0.1192,1.4961
finite-horizon,value iteration algorithm,-0.4574,1.7073
optimal policy,value iteration algorithm,-0.3162,1.6225
non-stationary,value iteration algorithm,0.1247,1.3231
cumulative intermediate reward,heuristic-based techniques,0.7967,0.6376
heuristic-based techniques,optimal policy,-0.1215,1.4976
heuristic-based techniques,non-stationary,0.5567,0.9416
heuristic-based techniques,value iteration algorithm,-0.2699,1.5937
