node1,node2,similarity,distance
Uniform-PAC,reinforcement learning,-0.3064,1.6164
Probably Approximately Correct,reinforcement learning,-0.197,1.5473
Probably Approximately Correct,Uniform-PAC,0.921,0.3974
high probability regret guarantees,reinforcement learning,-0.1127,1.4918
Uniform-PAC,high probability regret guarantees,0.5035,0.9965
Probably Approximately Correct,high probability regret guarantees,0.4646,1.0348
Uniform-PAC,finite-state episodic MDPs,-0.122,1.498
Uniform-PAC,optimal regret,0.8662,0.5172
PAC guarantees,Uniform-PAC,-0.2699,1.5937
Uniform-PAC,horizon,0.1712,1.2875
Probably Approximately Correct,finite-state episodic MDPs,0.1086,1.3352
Probably Approximately Correct,optimal regret,0.8821,0.4855
PAC guarantees,Probably Approximately Correct,-0.2538,1.5835
Probably Approximately Correct,horizon,0.1964,1.2678
finite-state episodic MDPs,high probability regret guarantees,-0.489,1.7257
high probability regret guarantees,optimal regret,0.2217,1.2477
PAC guarantees,high probability regret guarantees,-0.0673,1.4611
high probability regret guarantees,horizon,0.7279,0.7377
finite-state episodic MDPs,optimal regret,0.311,1.1739
PAC guarantees,finite-state episodic MDPs,0.1444,1.3081
finite-state episodic MDPs,horizon,-0.6679,1.8264
PAC guarantees,optimal regret,-0.2336,1.5708
horizon,optimal regret,-0.1013,1.4841
PAC guarantees,horizon,0.1059,1.3373
