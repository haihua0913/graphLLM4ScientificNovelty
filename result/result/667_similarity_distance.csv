node1,node2,similarity,distance
algorithm,multiarmed bandits,-0.1463,1.5141
EXP3,algorithm,0.0829,1.3543
algorithm,exploration parameters,0.6543,0.8315
EXP3,multiarmed bandits,-0.2692,1.5933
exploration parameters,multiarmed bandits,0.0405,1.3853
EXP3,exploration parameters,-0.26,1.5875
algorithm,regret,0.0866,1.3516
adversarial regime,algorithm,0.8374,0.5704
algorithm,learning rate,-0.7334,1.8619
algorithm,arm losses,-0.0128,1.4233
multiarmed bandits,regret,-0.006,1.4185
adversarial regime,multiarmed bandits,-0.1237,1.4991
learning rate,multiarmed bandits,-0.0181,1.427
arm losses,multiarmed bandits,-0.0406,1.4427
EXP3,regret,-0.0182,1.427
EXP3,adversarial regime,-0.0549,1.4525
EXP3,learning rate,-0.1759,1.5335
EXP3,arm losses,-0.1512,1.5173
adversarial regime,regret,-0.0053,1.418
exploration parameters,regret,-0.0258,1.4323
learning rate,regret,-0.1032,1.4854
arm losses,regret,-0.0371,1.4402
adversarial regime,exploration parameters,0.9066,0.4322
adversarial regime,learning rate,-0.4003,1.6735
adversarial regime,arm losses,-0.2038,1.5517
exploration parameters,learning rate,-0.3011,1.6132
arm losses,exploration parameters,-0.3775,1.6598
arm losses,learning rate,-0.0366,1.4399
algorithm,stochastic regime,-0.5929,1.7849
regret,stochastic regime,-0.184,1.5388
adversarial regime,stochastic regime,-0.1482,1.5154
learning rate,stochastic regime,0.8645,0.5206
arm losses,stochastic regime,0.0102,1.407
