node1,node2,similarity,distance
AdaGrad,Adaptive gradient-based optimizers,-0.0281,1.4339
Adam,Adaptive gradient-based optimizers,-0.0064,1.4187
Adaptive gradient-based optimizers,memory footprint,0.3224,1.1642
AdaGrad,Adam,0.981,0.195
AdaGrad,memory footprint,-0.0753,1.4665
Adam,memory footprint,-0.0497,1.4489
adaptive optimization method,memory footprint,0.5422,0.9568
memory footprint,sublinear memory cost,0.022,1.3985
memory footprint,per-parameter adaptivity,0.9852,0.172
adaptive optimization method,sublinear memory cost,-0.1552,1.52
adaptive optimization method,per-parameter adaptivity,0.5569,0.9414
per-parameter adaptivity,sublinear memory cost,-0.0515,1.4502
adaptive optimization method,convergence guarantees,0.3231,1.1636
adaptive optimization method,training very large deep models,-0.0976,1.4816
convergence guarantees,sublinear memory cost,-0.0083,1.4201
sublinear memory cost,training very large deep models,-0.2525,1.5827
convergence guarantees,per-parameter adaptivity,0.9362,0.3571
per-parameter adaptivity,training very large deep models,-0.1783,1.5351
convergence guarantees,training very large deep models,-0.3242,1.6274
