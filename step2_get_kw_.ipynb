{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调用get_kw_\n",
    "参数为：ref_file  为参考文献的文件\n",
    "cooccurrence_csv   要将提取的关键知识保存到哪\n",
    "processed_csv   要将已处理的参考文献保存到哪"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T07:22:45.781683Z",
     "start_time": "2025-01-17T07:22:45.761685Z"
    }
   },
   "source": [
    "import nltk\n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download('punkt')\n",
    "from collections import Counter\n",
    "import re\n",
    "from itertools import combinations\n",
    "from keybert import KeyLLM\n",
    "import openai\n",
    "from keybert.llm import OpenAI\n",
    "def extract_keywords(abstract):\n",
    "    prompt1 = \"\"\"\n",
    "    I have the following abstract:\n",
    "    [DOCUMENT]\n",
    "    \n",
    "    Based on the Abstract above, extract the keywords that best describe the topic of the abstract.\n",
    "    Make sure all extracted keywords or concepts appear verbatim in the text and do not exceed ten keywords.\n",
    "    Use the following format separated by commas:\n",
    "    <keywords>\n",
    "    \"\"\"\n",
    "    prompt2 = \"\"\"\n",
    "    **Instructions**:\n",
    "    - You are given an academic abstract.\n",
    "    - Your task is to extract the most important pieces of knowledge or concepts that:\n",
    "        - Describe the core topic of the paper.\n",
    "        - Represent the key knowledge central to the study.\n",
    "        - Are directly used in the paper’s research method, framework, or experimental design.\n",
    "        - Reflect the new theories, methods, or contributions introduced by the paper.\n",
    "        - Include key theories, frameworks, or technologies used.\n",
    "        - Represent the scientific or practical contribution to the field.\n",
    "    - All extracted items must appear verbatim in the abstract.\n",
    "    - Do not paraphrase, summarize, or add inferred content.\n",
    "    - Try to extract no more than ten distinct items.\n",
    "    - Return the final result as a single line, separated by English commas.\n",
    "    **Now process the following abstract:**\n",
    "    I have the following abstract: \n",
    "    [DOCUMENT]\n",
    "    Based on the abstract above, extract the knowledge that best describes the topic of the abstract.\n",
    "    Prioritize any concepts that relate to the paper’s innovation, methodology, and research contribution.\n",
    "    Make sure all extracted knowledge or concepts appear verbatim in the text and try not to exceed ten pieces.\n",
    "    Use the following format separated by commas:\n",
    "    <keywords>\n",
    "    \"\"\"\n",
    "    prompt = prompt1\n",
    "\n",
    "    base_url = \n",
    "    api_key = \n",
    "    # Create your LLM\n",
    "    client  = openai.OpenAI(base_url='https://open.api.gu28.top/v1',\n",
    "        # required but ignored\n",
    "        api_key=api_key)\n",
    "    \n",
    "    #model=\"llama3.3:70b-instruct-q2_k\"  gpt-4o\n",
    "    llm = OpenAI(client,model=\"gpt-4o\",prompt=prompt,chat=True)\n",
    "    # Load it in KeyLLM\n",
    "    kw_model = KeyLLM(llm)\n",
    "    keywords = kw_model.extract_keywords(abstract,check_vocab=True)\n",
    "    kws = keywords[0]\n",
    "    print(kws)\n",
    "    return kws\n",
    "\n",
    "def analyze_cooccurrence_standardized(sentences, keywords):\n",
    "    co_occurrence_counter = Counter()\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    # 预编译正则表达式模式\n",
    "    keyword_patterns = {kw: re.compile(r'\\b' + re.escape(kw.lower()) + r'\\b') for kw in keywords}\n",
    "    \n",
    "    for i in range(num_sentences):\n",
    "        # 确定窗口范围\n",
    "        if i == 0:\n",
    "            window_sentences = sentences[i:i + 2]  # 第一个句子，包含第1和第2句\n",
    "        elif i == num_sentences - 1:\n",
    "            window_sentences = sentences[i - 1:i + 1]  # 最后一个句子，包含倒数第2和第5句\n",
    "        else:\n",
    "            window_sentences = sentences[i - 1:i + 2]  # 中间句子，包含前一句、当前句、后一句\n",
    "        \n",
    "        window_text = ' '.join(window_sentences).lower()\n",
    "        \n",
    "        # 找出窗口中出现的关键词\n",
    "        present_keywords = [kw for kw in keywords if keyword_patterns[kw].search(window_text)]\n",
    "        \n",
    "        # 统计所有可能的关键词对\n",
    "        for pair in combinations(present_keywords, 2):\n",
    "            sorted_pair = tuple(sorted(pair))\n",
    "            co_occurrence_counter[sorted_pair] = 1\n",
    "    \n",
    "    return co_occurrence_counter\n",
    "\n",
    "def liucheng(abstract):\n",
    "    keyword_list = extract_keywords(abstract)\n",
    "    sentences = nltk.sent_tokenize(abstract)\n",
    "    rst = analyze_cooccurrence_standardized(sentences,keyword_list)\n",
    "    return rst"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T16:02:10.587954Z",
     "start_time": "2025-01-15T16:02:06.396559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "abstract = \"\"\"\n",
    "Large neural networks can now generate jokes, but do they really \"understand\" humor? We challenge AI models with three tasks derived from the New Yorker Cartoon Caption Contest: matching a joke to a cartoon, identifying a winning caption, and explaining why a winning caption is funny. These tasks encapsulate progressively more sophisticated aspects of \"understanding\" a cartoon; key elements are the complex, often surprising relationships between images and captions and the frequent inclusion of indirect and playful allusions to human experience and culture. We investigate both multimodal and language-only models: the former are challenged with the cartoon images directly, while the latter are given multifaceted descriptions of the visual scene to simulate human-level visual understanding. We find that both types of models struggle at all three tasks. For example, our best multimodal models fall 30 accuracy points behind human performance on the matching task, and, even when provided ground-truth visual scene descriptors, human-authored explanations are preferred head-to-head over the best machine-authored ones (few-shot GPT-4) in more than 2/3 of cases. We release models, code, leaderboard, and corpus, which includes newly-gathered annotations describing the image's locations/entities, what's unusual in the scene, and an explanation of the joke.\n",
    "\"\"\"\n",
    "rst = extract_keywords(abstract)\n",
    "print(rst)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Large neural networks', 'jokes', 'humor', 'AI models', 'New Yorker Cartoon Caption Contest', 'understanding', 'cartoon', 'visual understanding', 'multimodal models', 'language-only models']\n",
      "['Large neural networks', 'jokes', 'humor', 'AI models', 'New Yorker Cartoon Caption Contest', 'understanding', 'cartoon', 'visual understanding', 'multimodal models', 'language-only models']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:06:30.023791Z",
     "start_time": "2025-01-17T02:06:30.009789Z"
    }
   },
   "source": [
    "# 加载已有的共现关系\n",
    "def load_cooccurrence(csv_file):\n",
    "    if os.path.exists(csv_file):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        co_occurrence_counter = Counter()\n",
    "        #print(csv_file)\n",
    "        for _, row in df.iterrows():\n",
    "            pair = tuple(sorted([row['Keyword1'], row['Keyword2']]))\n",
    "            co_occurrence_counter[pair] = row['Count']\n",
    "        print(f\"已加载共现关系，共有 {len(co_occurrence_counter)} 个关键词对。\")\n",
    "    else:\n",
    "        co_occurrence_counter = Counter()\n",
    "        print(\"未找到共现关系文件，初始化一个新的计数器。\")\n",
    "    return co_occurrence_counter\n",
    "\n",
    "def save_cooccurrence(co_occurrence_counter, csv_file):\n",
    "    data = {\n",
    "        'Keyword1': [pair[0] for pair in co_occurrence_counter.keys()],\n",
    "        'Keyword2': [pair[1] for pair in co_occurrence_counter.keys()],\n",
    "        'Count': [count for count in co_occurrence_counter.values()]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"共现关系已保存到 '{csv_file}'。\")\n",
    "def load_processed_titles(processed_csv):\n",
    "    \"\"\"\n",
    "    从 CSV 文件加载已处理摘要的标题。\n",
    "\n",
    "    Args:\n",
    "        processed_csv (str): 存储已处理标题的 CSV 文件路径。\n",
    "\n",
    "    Returns:\n",
    "        set: 已处理标题的集合。\n",
    "    \"\"\"\n",
    "    if os.path.exists(processed_csv):\n",
    "        processed_df = pd.read_csv(processed_csv)\n",
    "        processed_titles = set(processed_df['Title'].tolist())\n",
    "        print(f\"已加载 {len(processed_titles)} 个已处理的摘要标题。\")\n",
    "    else:\n",
    "        processed_titles = set()\n",
    "        print(\"未找到已处理摘要记录文件，初始化一个新的记录。\")\n",
    "    return processed_titles\n",
    "def save_processed_titles(new_titles, processed_csv):\n",
    "    \"\"\"\n",
    "    将新增处理的标题保存到 CSV 文件。\n",
    "\n",
    "    Args:\n",
    "        new_titles (list): 新处理的标题列表。\n",
    "        processed_csv (str): 存储已处理标题的 CSV 文件路径。\n",
    "    \"\"\"\n",
    "    if os.path.exists(processed_csv):\n",
    "        processed_df = pd.read_csv(processed_csv)\n",
    "    else:\n",
    "        processed_df = pd.DataFrame(columns=['Title'])\n",
    "    \n",
    "    # 合并新处理的标题\n",
    "    new_df = pd.DataFrame({'Title': new_titles})\n",
    "    processed_df = pd.concat([processed_df, new_df], ignore_index=True)\n",
    "    processed_df.to_csv(processed_csv, index=False)\n",
    "    print(f\"已更新 {len(new_titles)} 个已处理的摘要标题。\")\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:06:33.046883Z",
     "start_time": "2025-01-17T02:06:33.030880Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "def get_kw_(ref_file,cooccurrence_csv,processed_csv):\n",
    "    # 加载已处理标题\n",
    "    processed_titles = load_processed_titles(processed_csv)\n",
    "    # 加载CSV文件\n",
    "    df = pd.read_csv(ref_file)\n",
    "    titles = df['title'].dropna().tolist()\n",
    "    abstracts = df['abstract'].dropna().tolist()\n",
    "    # 加载共现关系\n",
    "    co_occurrence_counter = load_cooccurrence(cooccurrence_csv)\n",
    "    # 新处理的标题\n",
    "    new_processed_titles = []\n",
    "    for title, abstract in zip(titles, abstracts):\n",
    "        if title in processed_titles:\n",
    "            continue  # 跳过已处理的摘要\n",
    "        #print(f'处理到{title}')\n",
    "        # 睡眠3秒\n",
    "        time.sleep(1)\n",
    "        #print(abstract)\n",
    "        co_occurrence_counter1 = liucheng(abstract)\n",
    "        if co_occurrence_counter == None:\n",
    "            co_occurrence_counter = co_occurrence_counter1\n",
    "        else:\n",
    "            co_occurrence_counter += co_occurrence_counter1\n",
    "        # 记录新处理的标题\n",
    "        new_processed_titles.append(title)\n",
    "    # 更新已处理的标题记录\n",
    "    if new_processed_titles:\n",
    "        save_processed_titles(new_processed_titles, processed_csv)\n",
    "    # 保存共现关系到 CSV 文件\n",
    "    save_cooccurrence(co_occurrence_counter, cooccurrence_csv)\n",
    "    print(\"处理完成。\")\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获得所有的共现关系"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T14:36:32.626915Z",
     "start_time": "2025-01-17T09:53:44.176956Z"
    }
   },
   "source": [
    "# get_kw_(ref_file='keybert/ref.csv', cooccurrence_csv = 'keybert/3-r-coocurrence.csv',processed_csv= 'keybert/processed_csv')\n",
    "\n",
    "# 读取all_focus.xlsx\n",
    "import pandas as pd\n",
    "import time\n",
    "df = pd.read_excel('data1/all_A_papers.xlsx')\n",
    "\n",
    "if 'co_occurrence_counter' not in df.columns:\n",
    "    df['co_occurrence_counter'] = None\n",
    "    \n",
    "# 得到Title\n",
    "titles = df['title'].tolist()\n",
    "# 我想根据df的索引来，因为我不但要读titles还有读取abstract列\n",
    "#abstracts = df['Abstract'].tolist()\n",
    "# for循环，得到index title和abstract\n",
    "has_done = 1473\n",
    "for index,title in enumerate(titles):\n",
    "    if index <= has_done:\n",
    "        continue\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        get_kw_(ref_file=f'data1/{index}_ref.csv', cooccurrence_csv = f'data1/{index}_cooccurrence.csv',processed_csv= f'data1/{index}_processed.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        continue\n",
    "    # 判断索引为index，的co_occurrence_counter是否不为空，如果不为空就continue\n",
    "    if pd.notna(df.loc[index,'co_occurrence_counter']):\n",
    "        continue\n",
    "    else:\n",
    "        # 读取{index}_paper.csv文件，\n",
    "        df1 = pd.read_csv(f'data1/{index}_paper.csv')\n",
    "        # 得到abstract列的第一行\n",
    "        abstract = df1['abstract'][0]\n",
    "        # 判断abstract是否为空，或者为nan，如果是就continue\n",
    "        if pd.isna(abstract) or not abstract:\n",
    "            continue\n",
    "        # 设置2s休眠\n",
    "        co_occurrence_counter = liucheng(abstract)\n",
    "        # 在all_focus.xlsx中加入新，为co_occurrence_counter，然后报错格式为Counter的co_occurrence_counter\n",
    "        df.loc[index,'co_occurrence_counter'] = str(co_occurrence_counter)\n",
    "        df.to_excel('data1/all_A_papers.xlsx',index=False)\n",
    "\"\"\"\n",
    "# 需要读取解析的时候使用eval函数\n",
    "from collections import Counter\n",
    "df = pd.read_excel('data/all_focus.xlsx')\n",
    "# 确保所有值都是字符串（跳过 NaN 或非字符串）\n",
    "def safe_eval(cell):\n",
    "    if isinstance(cell, str):\n",
    "        return eval(cell, {'Counter': Counter})\n",
    "    return cell\n",
    "# 使用 eval() 解析字符串恢复为 Counter\n",
    "df['co_occurrence_counter'] = df['co_occurrence_counter'].apply(safe_eval)\n",
    "# 打印解析后的结果\n",
    "print(df['co_occurrence_counter'][0])  # 输出: Counter({'This': 1, 'is': 1, 'an': 1, 'example': 1, 'abstract.': 1})\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未找到已处理摘要记录文件，初始化一个新的记录。\n",
      "未找到共现关系文件，初始化一个新的计数器。\n",
      "['differential privacy', 'empirical privacy estimate', 'private machine learning', 'auditing mechanisms', 'adversarial dataset', 'model updates', 'federated learning', 'tight composition theorems', 'privacy leakage', 'implementation bugs']\n",
      "['differentially private', 'privacy loss', 'privacy auditing', 'federated settings', 'Gaussian mechanism', 'FL benchmark datasets', 'adversarial threat models']\n",
      "['machine learning models', 'personal data ownership', 'intellectual property rights', 'statistical techniques', 'training data', 'adversarial tools', 'legal settings', 'low false-positive', 'offline scenarios']\n",
      "['privacy guarantee', 'differentially private', 'machine learner', 'privacy loss', 'poisoning attacks', 'membership inference', 'privacy violations', 'implementation errors']\n",
      "['Federated Learning', 'model updates', 'attacks', 'private information', 'Differential Privacy', 'noise', 'threat model', 'CANIFE', 'vision models', 'language models']\n",
      "['composition', 'differential privacy', 'privacy amplification', 'subsampling', 'analyses', 'data', 'concepts', 'proofs', 'key results']\n",
      "['Differentially Private SGD', 'privacy guarantees', 'privacy budget', 'Bayesian method', 'hypothesis testing', 'membership inference attacks']\n",
      "['Differential Privacy', 'DP-SGD', 'noise', 'image classification', 'hyper-parameter tuning', 'Wide-ResNet', 'CIFAR-10', 'NFNet-F3', 'ImageNet', 'accuracy gap']\n",
      "['Differential Privacy', 'provable privacy guarantees', 'machine learning', 'auditing', 'differentially private algorithms', 'flaws', 'open source implementation', 'deep learning algorithm']\n",
      "['membership inference attack', 'machine learning model', 'training dataset', 'true-positive rate', 'false-positive rates', 'Likelihood Ratio Attack', 'prior attacks']\n",
      "['differential privacy', 'privacy accounting', 'Renyi DP', 'privacy profiles', 'PLD formalism', 'characteristic function', 'adaptive composition', 'analytical Fourier accountant', 'Gaussian quadrature']\n",
      "['privacy-preserving machine learning', 'differential privacy', 'Laplace mechanism', 'PATE framework', 'Ghazi et al.', 'randomized response mechanism', 'Laplace noise', 'Bayesian inference', 'semi-supervised learning', 'memorization attacks']\n",
      "['privacy guarantees', 'differentially private', 'privacy loss random variables', 'fast algorithm', 'privacy curve', 'DP-SGD', 'Abadi et al. (2016)', 'Koskela et al. (2021)', 'running time', 'memory']\n",
      "['differentially private', 'machine learning', 'cryptographic game', 'adversary', 'dataset', 'privacy analysis', 'DP-SGD', 'neural networks', 'privacy', 'theoretical upper bound']\n",
      "['Differentially Private SGD', 'privacy in practice', 'data poisoning attacks', 'realistic privacy attacks', 'data poisoning', 'specific mechanism', 'quantitative', 'empirical approach', 'privacy afforded', 'differentially private algorithms']\n",
      "['Conditional Mutual Information', 'generalization properties', 'machine learning algorithms', 'information-theoretic framework', 'uniform convergence bounds', 'adaptive data analysis', 'VC dimension', 'compression schemes', 'differential privacy']\n",
      "['differentially private', 'sample-accurate', 'adaptive data analysis', 'statistical queries', 'transfer theorem', 'posterior distribution', 'transcript of the interaction', 'thought experiment', 'sample accuracy', 'resampled']\n",
      "['Sampled Gaussian Mechanism', 'subsampling', 'additive Gaussian noise', 'privacy amplification by sampling', 'Renyi Differential Privacy', 'differential privacy', 'machine learning applications', 'closed-form bound']\n",
      "['Differentially private', 'machine learning', '$(\\\\varepsilon', '\\\\delta)$-DP', 'numerical accountant', 'privacy loss', 'subsampled multidimensional Gaussian mechanism', 'stochastic gradient descent', 'fast Fourier transform', 'Python code', 'Github']\n",
      "['Differential privacy', 'privacy budget', 'privacy-preserving machine learning', 'iterative learning procedures', 'logistic regression', 'neural network models', 'privacy loss', 'inference attacks', 'utility-privacy trade-offs', 'privacy guarantees']\n",
      "['DP-Finder', 'differential privacy', 'lower bounds', 'counterexample', 'privacy violation', 'correlated sampling', 'optimization objective', 'randomized algorithms', 'obfuscate', 'numerical optimizers']\n",
      "['subsampling', 'differential privacy', 'Renyi Differential Privacy', 'RDP parameters', 'randomized mechanism', 'subsampling probability', 'moments accounting', 'Gaussian mechanism']\n",
      "['differential privacy', 'algorithms', 'privacy definition', 'counterexamples', 'incorrect algorithms', 'development process', 'statistical tests', 'violations', 'evaluation', 'usefulness']\n",
      "['Gaussian mechanism', 'differentially private data analysis', 'high privacy regime', 'low privacy regime', 'variance formula', 'Gaussian cumulative density function', 'tail bound approximation', 'optimal Gaussian mechanism', 'adaptive estimation techniques', 'high-dimensional regime']\n",
      "['adaptive data reuse', 'differential privacy', 'algorithmic stability', 'empirical answers', 'low variance', 'noise scaled', 'statistical queries']\n",
      "['statistical analyses', 'adaptive', 'overfitting', 'false discovery', 'queries', 'samples', 'expectations', 'differential privacy', 'generalization guarantees']\n",
      "['Rényi divergence', 'relaxation of differential privacy', 'privacy definition', 'privacy loss', 'differential privacy', 'analytical tool', 'composition', 'heterogeneous mechanisms']\n",
      "['membership inference attack', 'machine learning models', 'black-box access', 'training dataset', 'adversarial use', 'inference model', 'predictions', 'commercial \"machine learning as a service\" providers', 'classification models', 'privacy perspective']\n",
      "['neural networks', 'training', 'datasets', 'sensitive information', 'models', 'algorithmic techniques', 'differential privacy', 'deep neural networks', 'privacy budget']\n",
      "['deep residual networks', 'training', 'architecture', 'ResNet blocks', 'novel architecture', 'wide residual networks', 'accuracy', 'efficiency', 'state-of-the-art', 'ImageNet']\n",
      "['approximate differential privacy', 'adaptive hypothesis testing', 'p-value corrections', 'approximate max-information', 'product distribution', '(∈', 'δ)-differential privacy', 'max-information bounds']\n",
      "['Concentrated Differential Privacy', 'Differential Privacy', 'accuracy', 'cumulative privacy loss', 'multiple computations']\n",
      "['statistical validity', 'nonadaptive model', 'generalization error', 'adaptive data analysis', 'statistical queries', 'low-sensitivity queries', 'optimization queries', 'differential privacy', 'stability guarantees']\n",
      "['privacy risks', 'summary statistics', 'SNP allele frequencies', 'genome-wide association study', 'distorted summary statistics', 'ℓ1 norm', 'reference pool', 'Gaussian', 'Bernouilli', 'differential privacy']\n",
      "['overfitting', 'data analysis', 'adaptive', 'holdout set', 'hypotheses', 'algorithm', 'differential-privacy', 'statistical validity', 'approximate max-information']\n",
      "['statistical inference', 'spurious scientific discoveries', 'adaptive data analysis', 'false discovery rate', 'validation techniques', 'multiple hypothesis testing', 'expectations', 'empirical estimators', 'privacy preservation', 'perturbing and coordinating the estimates']\n",
      "['Convex empirical risk minimization', 'differentially private', 'Lipschitz', 'strongly convex', 'polynomial time', '(ε', '(ε', 'δ)-differential privacy', 'quadratic functions.']\n",
      "['Differential Privacy', 'privacy-preserving data analysis', 'query-release problem', 'mechanism design', 'machine learning', 'distributed databases', 'data streams']\n",
      "['differentially private mechanisms', 'privacy level', 'privatization mechanisms', 'privacy degradation', 'number of queries', 'upper bound', 'hypothesis testing', 'data processing inequality', 'state of the art']\n",
      "['high-density single nucleotide polymorphism', 'genomic DNA mixture', 'simulations', 'genomic mixtures', 'trace contributors', 'allele frequency', 'genotype counts', 'genome-wide association studies']\n",
      "['binary codes', 'fingerprinting', 'digital documents', 'pirates', 'Boneh--Shaw', 'length', 'lower bounds', 'error probabilities', 'randomized fingerprint codes', 'binary alphabet']\n",
      "['differential privacy', 'privacy', 'statistical databases', 'adversaries', 'Bayesian adversary', 'inferences', '(epsilon', 'delta)-differential privacy', 'Dwork and McSherry', 'formulation', 'parameters']\n",
      "['privacy-preserving', 'statistical databases', 'trusted server', 'query function', 'random noise', 'noisy sums', 'sensitivity', 'standard deviation', 'indistinguishability', 'interactive sanitization mechanisms']\n",
      "['fingerprinting', 'digital data', 'code-words', 'unauthorized copy', 'distributor', 'collusion', 'fingerprinted objects', 'users', 'identities', 'solution']\n",
      "['survey technique', 'reliability of responses', 'sensitive interview questions', 'privacy', 'randomizing', 'group membership', 'maximum likelihood estimates', 'sample size', 'parameter p']\n",
      "已更新 45 个已处理的摘要标题。\n",
      "共现关系已保存到 'data1/1474_cooccurrence.csv'。\n",
      "处理完成。\n",
      "['differentially private', 'machine learning', 'auditing', 'single training run', 'parallelism', 'training examples', 'statistical generalization', 'group privacy', 'black-box', 'white-box']\n",
      "未找到已处理摘要记录文件，初始化一个新的记录。\n",
      "未找到共现关系文件，初始化一个新的计数器。\n",
      "['Quantization Model', 'neural scaling laws', 'power law', 'loss', 'data size', 'emergence', 'Quantization Hypothesis', 'quanta', 'language models']\n",
      "['neural network', 'modular arithmetic', 'grokking', 'fully-connected', 'gradient descent', 'MSE loss function', 'regularization', 'feature maps', 'analytic expressions', 'interpretability']\n",
      "['Broken Neural Scaling Law', 'deep neural networks', 'scaling behavior', 'functional form', 'extrapolation', 'nonmonotonic transitions', 'double descent', 'inflection points', 'arithmetic', 'predictability']\n",
      "['grokking', 'loss landscapes', 'neural networks', 'training data', 'test losses', 'model weight norm', 'LU mechanism', 'representation learning', 'algorithmic datasets']\n",
      "['neural power-law scaling relations', 'deep learning', 'scaling laws', 'reinforcement learning', 'AlphaZero', 'Elo rating', 'playing strength', 'Connect Four', 'Pentago', 'sample efficient']\n",
      "['emergent phenomena', 'deep learning', 'sparse parity', 'computational problem', 'neural networks', 'phase transitions', 'training curves', 'iterations', 'Fourier gap', 'population gradient']\n",
      "['language models', 'performance', 'sample efficiency', 'downstream tasks', 'unpredictable phenomenon', 'emergent abilities', 'smaller models', 'larger models', 'extrapolating', 'scaling']\n",
      "['Language models', 'capabilities', 'BIG-bench', 'tasks', 'performance', 'model sizes', 'human expert raters', 'social bias', 'scale', 'prompting']\n",
      "['in-context learning', 'training data', 'distributional properties', 'burstiness', 'rarely occurring classes', 'natural language', 'naturalistic data', 'skewed Zipfian distribution', 'recurrent models']\n",
      "['Large language models', 'few-shot learning', 'Pathways Language Model PaLM', 'state-of-the-art', 'multi-step reasoning tasks', 'BIG-bench benchmark', 'multilingual tasks', 'bias and toxicity', 'ethical considerations']\n",
      "['model size', 'number of tokens', 'transformer language model', 'compute budget', 'undertrained', 'scaling language models', 'training data', 'compute-optimal', 'downstream evaluation tasks', 'MMLU benchmark']\n",
      "['parameter count', 'computational requirement', 'scaling laws', 'Routing Networks', 'language models', 'routing architectures', 'Effective Parameter Count', 'scaling coefficients', 'routing techniques', 'experts']\n",
      "['LaMDA', 'Transformer-based', 'neural language models', 'dialog', 'safety', 'factual grounding', 'external knowledge sources', 'human values', 'model scaling', 'annotated data']\n",
      "['generalization', 'neural networks', 'small algorithmically generated datasets', 'data efficiency', 'memorization', 'speed of learning', 'grokking', 'overfitting', 'dataset size', 'overparametrized neural networks']\n",
      "['Language modelling', 'Transformer-based language model', 'model scales', '280 billion parameter model', 'Gopher', 'reading comprehension', 'fact-checking', 'logical and mathematical reasoning', 'bias and toxicity', 'AI safety']\n",
      "['Vision Transformer', 'scaling properties', 'Transformer language models', 'error rate', 'data', 'compute', 'two billion parameters', 'ImageNet', 'few-shot transfer']\n",
      "['machine learning', 'experiments', 'extrapolated', 'size of the model', 'size of the problem', 'AlphaZero', 'Hex', 'performance', 'compute', 'agent']\n",
      "['empirical scaling laws', 'transfer learning', 'unsupervised', 'fine-tuning', 'neural networks', 'pre-trained', 'cross-entropy loss', 'power-law', 'generality', 'compute']\n",
      "['scaling laws', 'cross-entropy loss', 'autoregressive Transformers', 'power-law', 'multimodal image$\\\\leftrightarrow$text models', 'mathematical problem solving', 'YFCC100M image distribution', 'mutual information', 'ImageNet classification', 'downstream tasks']\n",
      "['GPT-3', 'language models', 'few-shot performance', 'task-agnostic', 'fine-tuning', 'NLP tasks', 'autoregressive', 'translation', 'question-answering', 'societal impacts']\n",
      "['language model performance', 'cross-entropy loss', 'power-law', 'model size', 'dataset size', 'compute', 'overfitting', 'training speed', 'sample-efficient', 'optimally compute-efficient']\n",
      "['generalization error', 'neural networks', 'dataset size', 'functional form', 'model scaling', 'language tasks', 'small- to large-scale models', 'data scales']\n",
      "['Deep learning', 'model architecture search', 'training set size', 'computational scale', 'model accuracy', 'generalization error', 'model size', 'machine learning domains', 'computational scaling']\n",
      "['Transformer', 'attention mechanisms', 'sequence transduction', 'encoder-decoder', 'machine translation', 'BLEU score', 'WMT 2014 English-to-German', 'WMT 2014 English-to-French', 'parallelizable', 'English constituency parsing']\n",
      "['handwritten characters', 'computational model', 'deep learning algorithms', 'probabilistic learning', 'humanlike performance', 'generalize', 'one-shot classification', 'Bayesian criterion', 'visual Turing tests', 'creative generalization']\n",
      "['Recall-Oriented Understudy for Gisting Evaluation', 'summary', 'n-gram', 'ROUGE-N', 'ROUGE-L', 'ROUGE-W', 'ROUGE-S', 'Document Understanding Conference']\n",
      "['machine translation', 'automated understudy', 'quick', 'inexpensive', 'language-independent', 'marginal cost', 'substitutes', 'skilled human judges']\n",
      "['cross-entropy loss', 'supervised neural machine translation', 'power law', 'training data', 'non-embedding parameters', 'practical implications', 'BLEU', 'large scale models', 'ROI', 'low-resource language pairs']\n",
      "['tiny images', 'unsupervised training', 'deep generative models', 'multi-layer generative model', 'human visual cortex', 'parallelization algorithm', 'object recognition', 'CIFAR-10', 'CIFAR-100']\n",
      "['articulated bar flail', 'shearing edges', 'shredding materials', 'shredder cylinder', 'rotatable shaft', 'shredder apparatus', 'parallel axes', 'conveyer apparatus', 'inclined converging conveyer belts', 'transport of articles']\n",
      "['gradient-based learning', 'handwritten character recognition', 'graph transformer networks', '2D shapes', 'online handwriting recognition', 'bank cheque']\n",
      "['fundamental laws', 'philosophers', 'controversy', 'scientists', 'astrophysicists', 'elementary particle physicists', 'logicians', 'mathematicians']\n",
      "已更新 32 个已处理的摘要标题。\n",
      "共现关系已保存到 'data1/1475_cooccurrence.csv'。\n",
      "处理完成。\n",
      "['emergent abilities', 'large language models', 'unpredictability', 'model behavior', 'continuous metrics', 'performance', 'scaling AI models', 'InstructGPT/GPT-3', 'BIG-Bench']\n",
      "未找到已处理摘要记录文件，初始化一个新的记录。\n",
      "未找到共现关系文件，初始化一个新的计数器。\n",
      "['prediction with expert advice', 'strongly convex', 'bounded losses', 'regret', 'variance', 'Exponentially Weighted Average', 'online to batch', 'corrupted losses', 'online selective sampling', 'online learning with abstention']\n",
      "['online active learning', 'streaming instances', 'statistical learning theory', 'queries', 'prediction errors', 'hypothesis space', 'Tsybakov noise', 'label complexity', 'bounded regret', 'tradeoff']\n",
      "['active learning', 'expert advice', 'online learning', 'greedy forecaster', 'active query strategies', 'noisy scenarios', 'robust active learning']\n",
      "['electronic form', 'LATEX', 'math displays', 'accent marks']\n",
      "['low-power distributed sensors', 'noisy measurements', 'unknown target function', 'local communication', 'best-response dynamics', 'game theory', 'empirical processes', 'denoising', 'agnostic active learning', 'active or passive learning']\n",
      "['Active learning', 'supervised machine learning', 'passive learning', 'labels', 'disagreement-based active learning', 'theoretical benefits', 'active learning algorithms', 'performance', 'theorems', 'pedagogical']\n",
      "['online aggregation', 'predictions of experts', 'second-order regret bounds', 'Prod algorithm', 'polynomially weighted average algorithm', 'learning rates', 'excess losses', 'standard setting', 'confidences', 'bounded regret']\n",
      "['active learning with expert advice', 'learner', 'oracle', 'accurate prediction model', 'questions', 'online active learning', 'Exponentially Weighted Average Forecaster', 'Greedy Forecaster', 'Hannan consistency', 'experiments']\n",
      "['computationally efficient', 'noise tolerant algorithms', 'malicious noise model', 'adversarial label noise', 'polynomial-time algorithm', 'linear separators', 'uniform distribution', 'isotropic log-concave', 'active learning setting', 'label complexity']\n",
      "['Gaussian process', 'nonconvex functions', 'noisy observations', 'high-dimensional', 'variable selection', 'optimization', 'sample complexity', 'cumulative regret bounds', 'empirical evidence', 'benchmark optimization problems']\n",
      "['SGD', 'stochastic optimization', 'strongly convex', 'convergence rate', 'smooth problems', 'non-smooth problems', 'averaging', 'experimental results', 'open problems']\n",
      "['online algorithms', 'selective sampling', 'regularized least squares', 'base classifier', 'performance guarantees', 'gradient-based classifier', 'selective sampler', 'hybrid algorithms']\n",
      "['importance weighting', 'actively learning', 'binary classifiers', 'general loss functions', 'sampling bias', 'variance', 'label complexity bounds']\n",
      "['transition probabilities', 'rewards', 'Markov Decision Process', 'agent', 'optimal policy', 'environment', 'exploration', 'sensitivity', 'control', 'planning problems']\n",
      "['active learning', 'Gaussian Processes', 'spatial phenomena', 'sequential design', 'a priori design', 'exploration', 'exploitation', 'nonmyopic approach', 'nonstationary GPs', 'logarithmic sample complexity']\n",
      "['active learning', 'passive learning', 'sequential sampling', 'feedback systems', 'minimax analysis', 'classification error convergence', 'decision boundary regularity', 'noise conditions', 'd-dimensional feature spaces']\n",
      "['estimation problems', 'measurement process', 'actively controlled', 'inference task', 'performance guarantees', 'heuristic algorithms', 'adaptive measurement selection', 'sequential estimation problems', 'mutual information', 'computational simulations']\n",
      "['selective sampling algorithm', 'learning algorithm', 'classification', 'linear-threshold classification algorithms', 'randomized selective sampling algorithms', 'mistake bounds', 'semi-supervised algorithms', 'fully supervised counterparts', 'textual data', 'theoretical results']\n",
      "['active learning algorithm', 'noise', 'samples', 'i.i.d.', 'distribution', 'A2', 'sample complexity', 'ε-optimal classifier', 'threshold classifiers', 'homogeneous linear separators']\n",
      "['Prediction with expert advice', 'Tight bounds', 'Randomized prediction', 'Efficient forecasters', 'Prediction and playing games', 'Absolute loss', 'Logarithmic loss', 'Sequential investment', 'Linear classification']\n",
      "['sample complexity', 'active learning', 'distribution', 'input space', 'target hypothesis', 'desired accuracy']\n",
      "['active learning', 'passive learning', 'statistical analysis', 'error decay', 'nonparametric function classes', 'performance limits', 'practical algorithm', 'wireless sensor networks', 'fault line detection']\n",
      "['label efficient prediction', 'prediction with expert advice', 'forecaster', 'excess prediction error', 'constant predictor', 'queries', 'Hannan consistency', 'game-theoretic prediction models']\n",
      "['Active learning', 'Gaussian random field']\n",
      "['labels', 'on-line prediction model', 'algorithms', 'adversary arguments', 'tradeoffs', 'mistakes', 'recurrence', 'compute', 'optimally']\n",
      "['X 1', '...', 'X n', 'independent identically distributed', 'probability density', 'level set', 'smoothness', 'piecewise-polynomial estimators', 'local empirical excess masses', 'optimal rates of convergence', 'convex level sets', 'N-dimensional case']\n",
      "['feedforward neural networks', 'mixtures of Gaussians', 'locally weighted regression', 'learning architectures', 'statistically \"optimal\"']\n",
      "['salient data points', 'Bayesian learning framework', 'objective functions', 'expected informativeness', 'candidate measurements', 'hypothesis space', 'main weakness']\n",
      "['learner', 'sequence of trials', 'make few mistakes', 'known algorithms', 'weighted voting', 'compound algorithm', 'weighted majority algorithm', 'robust', 'error bounds']\n",
      "['multiclass prediction', 'expert advice', 'active learning', 'regret', 'Hedge', 'Exponential Weights', 'compactness', 'sample complexity', 'polynomial time algorithm']\n",
      "['domain shifts', 'label queries', 'hidden domains', 'online linear regression', 'oblivious adversaries', 'tight tradeoff', 'interleaving spans', 'non-linear regression', 'eluder dimension', 'adaptive adversaries']\n",
      "['online learning algorithm', 'selective sampling framework', 'active learning', 'regret', 'adversarial strategy', 'bound', 'multiple-teacher setting', 'labels', 'instances', 'Internet search problem']\n",
      "已更新 32 个已处理的摘要标题。\n",
      "共现关系已保存到 'data1/1476_cooccurrence.csv'。\n",
      "处理完成。\n",
      "['online prediction', 'binary sequence', 'expert advice', 'label-efficient', 'selective sampling', 'regret guarantees', 'exponentially weighted forecasters', 'label complexity', 'numerical experiments', 'minimax rates']\n",
      "未找到已处理摘要记录文件，初始化一个新的记录。\n",
      "未找到共现关系文件，初始化一个新的计数器。\n",
      "['procedural knowledge', 'control tasks', 'transformer architecture', 'low-level trainable actor', 'high-level sub-tasks', 'AlfWorld instruction following benchmark', 'generalization', 'human goal specifications']\n",
      "['SEEM', 'promptable', 'interactive model', 'segmenting', 'decoding mechanism', 'visual prompt', 'joint visual-semantic space', 'learnable memory prompts', 'text encoder', 'segmentation tasks']\n",
      "['large language models', 'GPT-4', 'OpenAI', 'general intelligence', 'artificial general intelligence', 'ChatGPT', \"Google's PaLM\", 'limitations', 'societal influences']\n",
      "['Transformer-based detector', 'grounded pre-training', 'open-set object detection', 'language-guided query selection', 'cross-modality fusion', 'referring expression comprehension', 'zero-shot benchmark', 'COCO', 'ODinW']\n",
      "['Large language models', 'grounding', 'embodied language models', 'multi-modal sentences', 'visual question answering', 'sequential robotic manipulation planning', 'captioning', 'PaLM-E', 'visual-language generalist', 'positive transfer']\n",
      "['Autonomous agents', 'generalist agents', 'open world', 'MineDojo', 'Minecraft', 'multimodal knowledge', 'simulation suite', 'open-ended tasks', 'video-language models', 'learned reward function.']\n",
      "['deep reinforcement learning (DRL)', 'sample efficiency', 'data richness', 'exploration-exploitation trade-off', 'behavior policy', 'Generalized Policy Iteration (GPI)', 'Generalized Data Distribution Iteration (GDI)', 'Arcade Learning Environment (ALE)', 'human normalized score (HNS)', 'operator-based versions']\n",
      "['Pretrained large language models', 'few-shot learners', 'zero-shot reasoners', 'reasoning tasks', 'MultiArith', 'GSM8K', 'PaLM']\n",
      "['video-language models', 'few-shot', 'video-to-text tasks', 'VidIL', 'image-language models', 'language model', 'video captioning', 'video question answering', 'video future event prediction']\n",
      "['language models', 'semantic knowledge', 'real-world experience', 'decision making', 'real-world grounding', 'pretrained skills', 'natural language actions', 'high-level semantic knowledge', 'low-level skills', 'robotic tasks']\n",
      "['Vision-Language Pre-training', 'VLP', 'vision-language tasks', 'noisy image-text pairs', 'BLIP', 'bootstrapping', 'synthetic captions', 'state-of-the-art results', 'generalization ability', 'zero-shot manner']\n",
      "['driving policy learning', 'data-efficiency', 'edge cases', 'interactive driving', 'data-driven simulation engine', 'inpainted ado vehicles', 'multi-agent interactions', 'policy learning methods', 'autonomous vehicle']\n",
      "['multi-environment Symbolic Interactive Language Grounding', 'SILG', 'grid-world environments', 'language grounding', 'generalization', 'shared model architecture', 'environment-specific architectures', 'recurrent state-tracking', 'entity-centric attention', 'pretrained LM']\n",
      "['semantic map', 'exploration', 'semantic search policy', 'natural language', 'modular method', 'SOTA performance', 'state tracking', 'spatial memory', 'expert trajectories', 'low-level instructions']\n",
      "['pretrained language models', 'Macaw', 'UnifiedQA', 'T5', 'zero-shot', 'Challenge300', 'parameters', 'limitations']\n",
      "['language', 'cultural knowledge', 'transmission', 'cultural learning', 'video games', 'iterated learning paradigm', 'natural language', 'strategies for success', 'AI systems']\n",
      "['Habitat 2.0', 'virtual robots', 'interactive 3D environments', 'complex physics-enabled scenarios', 'deep reinforcement learning', 'ReplicaCAD', 'simulation', 'Home Assistant Benchmark', 'mobile manipulation']\n",
      "['Episodic Transformer', 'multimodal transformer', 'neural agents', 'natural language instructions', 'dynamic environments', 'visual observations', 'synthetic instructions', 'compositional tasks', 'ALFRED benchmark', 'task success rates']\n",
      "['Knowledge-intensive tasks', 'question answering', 'large inputs', 'ReadTwice', 'long-range dependencies', 'Transformers', 'memory table']\n",
      "['image representations', 'raw text', 'pre-training', 'zero-shot transfer', 'computer vision', 'caption', 'natural language', 'fine-grained object classification', 'ResNet-50']\n",
      "['natural language', 'control policies', 'multi-task environment', 'Messenger', 'free-form text manuals', 'entity symbols', 'EMMA', 'entity-conditioned attention', 'end-to-end differentiable', 'zero-shot generalization']\n",
      "['abstract plans', 'ALFWorld', 'TextWorld', 'ALFRED benchmark', 'simulator', 'text-based policies', 'visual environment', 'BUTLER agent', 'generalization']\n",
      "['Autonomous car racing', 'robotics', 'planning minimum-time trajectories', 'uncertain dynamics', 'learning-based approaches', 'high-fidelity physical car simulation', 'course-progress proxy reward', 'deep reinforcement learning', 'Gran Turismo Sport', 'realistic physics simulation']\n",
      "['reinforcement learning', 'offline RL', 'conservative Q-learning', 'Q-function', 'Bellman error', 'deep Q-learning', 'actor-critic', 'policy learning', 'discrete and continuous control domains', 'multi-modal data distributions']\n",
      "['pre-training', 'fine-tuning', 'few-shot performance', 'language models', 'GPT-3', 'autoregressive', 'translation', 'question-answering', 'cloze tasks', 'societal impacts']\n",
      "['Transformer models', 'Extended Transformer Construction', 'global-local attention', 'structured inputs', 'relative position encodings', 'Contrastive Predictive Coding', 'pre-training objective', 'natural language datasets']\n",
      "['Transformer-based models', 'self-attention', 'Longformer', 'attention mechanism', 'sequence length', 'local windowed attention', 'global attention', 'pretrained', 'Longformer-Encoder-Decoder', 'arXiv summarization dataset']\n",
      "['Atari games', 'reinforcement learning', 'RL algorithms', 'Agent57', 'human benchmark', 'exploratory', 'exploitative', 'adaptive mechanism', 'neural network', 'stable learning']\n",
      "['deep reinforcement learning', 'RL', 'automated driving tasks', 'autonomous driving agents', 'behavior cloning', 'imitation learning', 'inverse reinforcement learning', 'simulators', 'validate', 'robustify']\n",
      "['complex multi-object scenes', 'unsupervised object-oriented scene representation', 'spatial-attention', 'scene-mixture', 'generative latent variable model', 'probabilistic modeling framework', 'factorized object representations', 'parallel spatial-attention', 'Atari', '3D-Rooms']\n",
      "['ALFRED', 'benchmark', 'natural language instructions', 'egocentric vision', 'household tasks', 'expert demonstrations', 'interactive visual environments', 'high-level goals', 'low-level language instructions', 'baseline model']\n",
      "['reinforcement learning', 'generalisation', 'reading policy learner', 'language understanding', 'grounded policy learning', 'Read to Fight Monsters (RTFM)', 'procedurally generate', 'txt2$\\\\pi$', 'curriculum learning', 'coreference steps']\n",
      "['deep reinforcement learning', 'game play', 'simulated robotic control', 'model-free algorithms', 'deep Q-learning', 'policy gradients', 'Q-value policy gradients', 'rlpyt', 'Python', 'PyTorch']\n",
      "['BERT pretraining', 'hyperparameter choices', 'training data size', 'replication study', 'computationally expensive', 'GLUE', 'RACE', 'SQuAD', 'state-of-the-art results', 'design choices']\n",
      "['latent generative factors', 'mutual information', 'neural encoder', 'observations', 'benchmark', 'Atari 2600', 'representations', 'ground truth state variables', 'contrastive representation learning methods']\n",
      "['Reinforcement Learning', 'compositional', 'relational', 'hierarchical structure', 'representation learning', 'world knowledge', 'text corpora', 'decision making', 'instruction following', 'Natural Language Processing']\n",
      "['sample-efficient learning', 'approximate dynamic programming', 'Q-learning', 'actor-critic methods', 'bootstrapping error', 'Bellman backup operator', 'action selection', 'bootstrapping error accumulation reduction', 'continuous control tasks']\n",
      "['reinforcement learning', 'sample inefficient', 'reward shaping', 'natural language instructions', 'intermediate rewards', \"Montezuma's Revenge\", 'Atari video games', 'interaction time', 'benchmark', 'experiments']\n",
      "['Model-free deep reinforcement learning', 'Soft Actor-Critic', 'maximum entropy RL framework', 'off-policy actor-critic algorithm', 'sample complexity', 'hyperparameters', 'robotics tasks.']\n",
      "['navigation', 'spatial reasoning', 'Touchdown task', 'dataset', 'Street View environment', 'natural language', 'English instructions', 'spatial descriptions', 'demonstrations', 'methods']\n",
      "['Vision-language navigation', 'cross-modal grounding', 'Reinforced Cross-Modal Matching', 'reinforcement learning', 'matching critic', 'reasoning navigator', 'VLN benchmark dataset', 'generalizability', 'success rate performance gap']\n",
      "['perioperative anxiety', 'postoperative pain', 'non-pharmacological intervention', 'ambulatory surgery', 'serious games for health', 'CliniPup', 'SERES framework', 'ethnographic research', 'participatory design']\n",
      "['distributed training', 'RL agents', 'RNN-based', 'prioritized experience replay', 'parameter lag', 'representational drift', 'recurrent state staleness', 'Recurrent Replay Distributed DQN', 'Atari-57', 'DMLab-30']\n",
      "['humans in the loop', 'grounded language learning', 'BabyAI', 'extensible suite', 'levels of increasing difficulty', 'combinatorially rich synthetic language', 'heuristic expert agent', 'human teacher', 'baseline results', 'deep learning methods']\n",
      "['deep reinforcement-learning', 'language-conditional reward functions', 'instruction-conditional RL agents', 'reward models', 'expert examples', 'grid world', 'spatial relations', 'abstract arrangements']\n",
      "['value-based reinforcement learning', 'deep Q-learning', 'overestimated value estimates', 'actor-critic', 'Double Q-learning', 'target networks', 'overestimation bias', 'policy updates', 'OpenAI gym tasks']\n",
      "['AI2-THOR', 'visual AI research', '3D indoor scenes', 'AI agents', 'interact with objects', 'deep reinforcement learning', 'imitation learning', 'visual question answering', 'object detection', 'learning models of cognition']\n",
      "['natural language', 'transfer', 'reinforcement learning', 'generalized policy representations', 'textual descriptions', 'model-based RL', 'differentiable planning module', 'model-free component', 'factorized state representation', 'policy learning']\n",
      "['policy gradient methods', 'reinforcement learning', 'stochastic gradient ascent', 'proximal policy optimization', 'trust region policy optimization', 'sample complexity', 'simulated robotic locomotion', 'Atari game playing', 'online policy gradient methods']\n",
      "['interpretation', 'spatial references', 'contextual', 'spatial reasoning', 'simulated environment', 'agent', 'representation', 'instruction text', 'reinforcement learning', 'generalized value iteration']\n",
      "['task-oriented language grounding', '3D environments', 'natural language instructions', 'semantically meaningful representations', 'Gated-Attention mechanism', 'imitation learning', 'neural architecture', 'raw pixels', '3D game engine']\n",
      "['artificially intelligent technology', 'language', 'grounded and embodied', 'unsupervised learning', 'linguistic symbols', 'perceptual representations', 'semantic knowledge', 'natural language', 'physical world']\n",
      "['curiosity', 'intrinsic reward', 'sparse extrinsic reward', 'exploration', 'self-supervised inverse dynamics model', 'high-dimensional continuous state spaces', 'visual feature space', 'VizDoom', 'Super Mario Bros', 'generalization']\n",
      "['raw visual observations', 'text input', 'instruction execution', 'structured environment representations', 'reinforcement learning', 'contextual bandit', 'neural network agent', 'reward shaping', 'simulated environment', 'supervised learning']\n",
      "['Mask R-CNN', 'object instance segmentation', 'Faster R-CNN', 'segmentation mask', 'bounding box recognition', 'COCO suite of challenges', 'instance-level recognition.']\n",
      "['Stanford Question Answering Dataset', 'reading comprehension', 'Wikipedia articles', 'crowdworkers', 'constituency trees', 'logistic regression model', 'F1 score', 'human performance']\n",
      "['adaptive natural language interfaces', \"Wittgenstein's language games\", 'blocks world', 'compositionality', 'synonyms', 'semantic parsing model']\n",
      "['asynchronous gradient descent', 'deep reinforcement learning', 'optimization', 'neural network controllers', 'asynchronous variants', 'parallel actor-learners', 'asynchronous actor-critic', 'Atari domain', 'continuous motor control', 'visual input']\n",
      "['detection quality', 'multiple object tracking', 'realtime applications', 'Kalman Filter', 'Hungarian algorithm', 'tracking performance', 'accuracy', 'state-of-the-art online trackers', 'tracker updates']\n",
      "['neural sequence-to-sequence model', 'direction following', 'autonomous agents', 'alignment-based encoder-decoder', 'long short-term memory recurrent neural networks', 'natural language instructions', 'action sequences', 'observable world state', 'multi-level aligner', 'generalizable']\n",
      "['automatically extracted textual knowledge', 'control applications', 'stochastic player', 'game state', 'Monte-Carlo search framework', 'environment feedback', 'Civilization II', 'game manual', 'linguistically-informed game-playing agent', 'built-in AI']\n",
      "['machine learning', 'representations', 'reading to learn', 'semantic analysis', 'semantic abstract', 'generative model', 'multiple documents', 'transformed feature space', 'relational learning', 'generalization']\n",
      "['function approximation', 'reinforcement learning', 'value function', 'policy', 'gradient of expected reward', 'REINFORCE method', 'actor-critic methods', 'action-value', 'advantage function', 'policy iteration']\n",
      "['BERT', 'Bidirectional Encoder Representations', 'Transformers', 'pre-train', 'bidirectional representations', 'fine-tuned', 'state-of-the-art', 'question answering', 'language inference', 'natural language processing']\n",
      "['navigation', 'spatial reasoning', 'TOUCHDOWN task', 'dataset', 'real-life visual urban environment', 'goal position', 'natural language', 'hidden object', 'English instructions', 'spatial descriptions']\n",
      "['atomic nucleus', 'electromagnetic force', 'muon’s energy levels', 'hyperfine structure', 'nuclear force', 'weak force', 'capture', '1958']\n",
      "已更新 66 个已处理的摘要标题。\n",
      "共现关系已保存到 'data1/1477_cooccurrence.csv'。\n",
      "处理完成。\n",
      "['High sample complexity', 'Instruction manuals', 'Read and Reward', 'QA Extraction', 'Reasoning module', 'Atari games', 'A2C RL agent']\n",
      "未找到已处理摘要记录文件，初始化一个新的记录。\n",
      "未找到共现关系文件，初始化一个新的计数器。\n",
      "['ImageBind', 'joint embedding', 'modalities', 'image-paired data', 'zero-shot capabilities', 'cross-modal retrieval', 'arithmetic', 'state-of-the-art', 'few-shot recognition', 'vision models']\n",
      "['annotation cost', 'pretraining-finetuning', 'unsupervised pretraining', 'supervised finetuning', 'active finetuning', 'ActiveFT', \"Earth Mover's distance\", 'image classification']\n",
      "['self-supervised pretraining', 'contrastive learning', 'masked image modeling', 'part-aware representations', 'object parts', 'part-to-whole task', 'part-to-part task', 'object-level recognition', 'part-level recognition']\n",
      "['video transformers', 'large-scale datasets', 'VideoMAE', 'self-supervised video pre-training', 'video tube masking', 'masking ratio', 'video representations', 'small datasets', 'Kinetics-400']\n",
      "['hierarchically cascaded transformers', 'data efficiency', 'attribute surrogates learning', 'spectral tokens pooling', 'visual recognition', 'intrinsic image structures', 'few-shot learning', 'miniImageNet', 'HCTransformers']\n",
      "['Masked Feature Prediction', 'self-supervised pre-training', 'video models', 'Histograms of Oriented Gradients', 'feature descriptor', 'Transformer', 'MViTv2-L', 'Kinetics-400', 'AVA']\n",
      "['deep features', 'pretrained Vision Transformer', 'self-supervised ViT model', 'DINO-ViT', 'semantic information', 'zero-shot methodologies', 'co-segmentation', 'semantic correspondences', 'quantitative evaluation', 'unsupervised methods']\n",
      "['masked image modeling', 'visual tokenizer', 'self-supervised framework', 'online tokenizer', 'self-distillation', 'visual semantics', 'ImageNet-1K', 'image classification', 'object detection', 'semantic segmentation']\n",
      "['masked autoencoders', 'scalable self-supervised learners', 'computer vision', 'asymmetric encoder-decoder architecture', 'input image', 'reconstructs the original image', 'self-supervisory task', 'accelerate training', 'ImageNet-1K data', 'downstream tasks']\n",
      "['visualize', 'discrimination power', 'intermediate-layer visual patterns', 'DNN', 'training process', 'forward propagation', 'representation capacity', 'signal-processing behaviors', 'adversarial attacks', 'knowledge distillation']\n",
      "['3D scene understanding', 'spatio-temporal representation learning', 'self-supervised', '3D point clouds', 'invariant representation', 'synthetic', 'indoor', 'outdoor', '3D shape classification', '3D semantic segmentation']\n",
      "['active learning', 'Influence Selection for Active Learning', 'model performance', 'Untrained Unlabeled sample Influence Calculation', 'model-agnostic', 'task-agnostic', 'state-of-the-art performance', 'annotation cost', 'CIFAR10', 'VOC2012']\n",
      "['Convolutional neural networks', 'Vision Transformer', 'image classification', 'self-attention', 'residual connections', 'spatial localization', 'pretraining', 'dataset scale', 'transfer learning', 'MLP-Mixer']\n",
      "['data annotation', 'active learning', 'informative samples', 'unlabeled dataset', 'Temporal Output Discrepancy', 'sample loss', 'unlabeled samples', 'unsupervised learning']\n",
      "['pretrained language models', 'text generation', 'few-shot setting', 'random sampling', 'selection strategies', 'model performance', 'few-shot training instances', 'K-means clustering', 'data distribution', 'clustering-based approach']\n",
      "['Active learning', 'integer optimization', 'core subset', 'Wasserstein distance', 'Generalized Benders Decomposition', 'latent features', 'unsupervised learning', 'low budget regime', 'labeled data', 'unlabeled pool']\n",
      "['self-supervised learning', 'Vision Transformer', 'semantic segmentation', 'supervised ViTs', 'convnets', 'k-NN classifiers', 'ImageNet', 'momentum encoder', 'multi-crop training', 'small patches']\n",
      "['Deep learning', 'vision', 'natural language processing', 'computation', 'human labeling effort', 'informative and diverse subsets', 'submodular objective functions', 'balancing constraints', 'matroids', 'classification datasets']\n",
      "['active learning', 'object detection', 'instance-level uncertainty', 'MI-AOD', 'instance uncertainty learning', 'adversarial instance classifiers', 'multiple instance learning', 'image uncertainty', 'noisy instances', 'labeled sets']\n",
      "['self-supervised learning', 'Vision Transformers', 'ViT', 'training recipes', 'instability', 'MoCo v3', 'convolutional networks', 'benchmark', 'fundamental components', 'accuracy']\n",
      "['active learning', 'object detection', 'labeling costs', 'mixture density networks', 'probabilistic distribution', 'epistemic uncertainty', 'informativeness score', 'PASCAL VOC', 'MS-COCO']\n",
      "['computer vision', 'fixed set of predetermined object categories', 'image representations', 'zero-shot transfer', 'downstream tasks', 'computer vision datasets', 'ResNet-50', 'ImageNet', 'https://github.com/OpenAI/CLIP']\n",
      "['pretraining', '3D recognition', 'self-supervised learning', 'single-view depth scans', 'point cloud', 'voxel based model', 'object detection', 'semantic segmentation', 'object classification']\n",
      "['attention', 'image classification', 'visual transformers', 'convolution-free transformer', 'Imagenet', 'vision transformer', 'distillation token', 'convnet', 'teacher-student strategy', 'accuracy']\n",
      "['3D scene understanding', 'data-efficient learning', '3D point cloud', 'Contrastive Scene Contexts', 'state-of-the-art results', 'point-level correspondences', 'spatial contexts', 'instance segmentation', 'semantic segmentation']\n",
      "['Transformer architecture', 'natural language processing', 'computer vision', 'attention', 'convolutional networks', 'pure transformer', 'image patches', 'image classification', 'Vision Transformer (ViT)', 'image recognition benchmarks']\n",
      "['Active learning', 'model’s performance', 'information abundance', 'annotated datasets', 'deep active learning', 'sample annotation', 'DeepAL', 'learning capabilities']\n",
      "['contrastive learning framework', 'self-supervised contrastive methods', 'transferable visual representations', 'data augmentations', 'representational invariances', 'downstream task', 'separate embedding spaces', 'multi-head network', 'shared backbone', 'invariant and varying spaces']\n",
      "['self-supervised image representation learning', 'Bootstrap Your Own Latent', 'online network', 'target network', 'augmented view', 'negative pairs', 'state of the art', 'top-1 classification accuracy', 'ImageNet', 'ResNet-50 architecture']\n",
      "['multiple views', 'self-supervised representation learning', 'mutual information', 'data augmentation', 'downstream classification accuracy', 'unsupervised pre-training', 'ResNet-50', 'ImageNet classification', 'PASCAL VOC']\n",
      "['active learning', 'label-efficient algorithms', 'state relabeling adversarial active learning model', 'annotation information', 'labeled/unlabeled state information', 'representation generator', 'state discriminator', 'online uncertainty indicator', 'initially sampling algorithm']\n",
      "['Deep Neural Networks', 'fully supervised', 'autonomous driving', 'perception-based', 'unlabeled data', 'labeled by humans', 'high-quality annotation', 'active learning', 'data efficiency', 'scalable production system']\n",
      "['self-supervised learning', 'content-preserving transformations', 'contrastive loss', 'Generalized Data Transformations', 'invariance', 'distinctiveness', 'audio-visual representation learning', 'multi-modal self-supervision', 'state-of-the-art', 'video and audio classification']\n",
      "['Momentum Contrast', 'MoCo', 'unsupervised visual representation learning', 'contrastive learning', 'dynamic dictionary', 'moving-averaged encoder', 'ImageNet classification', 'transfer', 'downstream tasks']\n",
      "['block sub-image annotation', 'semantic segmentation', 'full-image annotations', 'crowdsourced', 'pixel-level annotations', 'weakly-supervised', 'spatial context', 'affordance relationships', 'inpaint', 'high-quality labels']\n",
      "['automated augmentation strategies', 'image classification', 'object detection', 'search phase', 'proxy task', 'distortion magnitude', 'CIFAR-10/100', 'SVHN', 'ImageNet', 'COCO datasets']\n",
      "['continual learning', 'regularization-based', 'external representation', 'uncertainty', 'Bayesian Neural Networks', 'learning rate', 'catastrophic forgetting', 'weight pruning', 'binary masks', 'object classification datasets']\n",
      "['active learning', 'deep neural networks', 'annotation', 'loss prediction module', 'task-agnostic', 'computationally inefficient', 'target losses', 'image classification', 'object detection', 'human pose estimation']\n",
      "['active learning', 'label-efficient algorithms', 'semi-supervised active learning', 'adversarial manner', 'variational autoencoder', 'adversarial network', 'mini-max game', 'latent space', 'image classification', 'semantic segmentation']\n",
      "['deep learning models', 'data-efficient', 'object recognition tasks', 'benchmark datasets', 'random subsets', 'generalize', 'CIFAR-10', 'ImageNet', 'semantic correlations']\n",
      "['Deep learning', 'image classification', 'labeled data', 'medical image diagnosis', 'active learning', 'convolutional neural network', 'ensemble-based methods', 'Monte-Carlo Dropout', 'predictive uncertainties', 'diabetic retinopathy dataset']\n",
      "['training data subset selection', 'active learning', 'diversity models', 'Facility-Location', 'Disparity-Min', 'computer vision', 'human labeling efforts', 'machine learning models', 'Convolutional Neural Networks', 'uncertainty sampling']\n",
      "['Active Learning', 'convolutional neural network', 'semantic segmentation', 'medical imaging', 'labeled data', 'Cost-Effective Active Learning', 'dropout', 'Monte Carlo sampling', 'pixel-wise uncertainty', 'source code']\n",
      "['CNNs', 'recognition', 'learning tasks', 'large dataset', 'supervised examples', 'active learning', 'empirical study', 'core-set selection', 'theoretical result', 'image classification']\n",
      "['dilation', 'dilated residual networks', 'image classification', 'spatial acuity', 'feature maps', 'gridding artifacts', 'object localization', 'semantic segmentation', 'degridding']\n",
      "['Cityscapes', 'urban street scenes', 'deep learning', 'semantic urban scene understanding', 'dataset', 'instance-level semantic labeling', 'stereo video sequences', 'high quality pixel-level annotations']\n",
      "['residual learning framework', 'residual networks', 'easier to optimize', 'ImageNet dataset', 'ILSVRC 2015', 'CIFAR-10', 'COCO object detection dataset', 'ILSVRC & COCO 2015 competitions']\n",
      "['Adam', 'first-order gradient-based optimization', 'stochastic objective functions', 'adaptive estimates', 'non-stationary objectives', 'sparse gradients', 'theoretical convergence properties', 'online convex optimization', 'AdaMax']\n",
      "['convolutional neural networks', 'image databases', 'computer vision', 'representations', 'inner layers', 'object detectors', 'scene classification', 'meaningful objects', 'object localization']\n",
      "['convolutional network depth', 'image recognition', 'convolution filters', 'ImageNet Challenge 2014', 'weight layers', 'classification', 'localisation', 'generalise', 'state-of-the-art results', 'ConvNet models']\n",
      "['active learning', 'computer vision', 'uncertainty measures', 'instance selection criteria', 'information density measure', 'object recognition', 'scene recognition', 'image classifications', 'unlabeled instances', 'querying outliers']\n",
      "['ImageNet LSVRC-2010', '1.2 million high-resolution images', 'top-5 error rate', '60 million parameters', '650', '000 neurons', 'non-saturating neurons', 'very efficient GPU implementation', 'dropout']\n",
      "['spectral clustering', 'clustering procedures', 'computational complexity', 'approximate spectral clustering', 'local transformation', 'mis-clustering rate', 'local k-means clustering', 'random projection trees', 'speedups', 'memory footprint']\n",
      "['ImageNet', 'WordNet', 'ontology of images', 'database', 'annotated images', 'semantic hierarchy', 'Amazon Mechanical Turk', 'object recognition', 'image classification']\n",
      "['active learning', 'classification problems', 'labeled training data', 'uncertainty sampling', 'uncertainty measure', 'multi-class', 'UCI repository', 'Caltech-101 dataset', 'scene categorization', 'computational overhead']\n",
      "['Gulf War Syndrome', 'factor analysis', 'symptoms', 'veterans', 'health effects', 'deployment', 'population-based survey', 'self reported symptoms', 'Gulf War illness']\n",
      "['spectral clustering', 'eigenvectors', 'matrices', 'data', 'Matlab', 'matrix perturbation theory', 'algorithm', 'clustering', 'experimental results']\n",
      "['Bayesian learning framework', 'objective functions', 'expected informativeness', 'candidate measurements', 'hypothesis space']\n",
      "['dataset', 'tiny colour images', 'unsupervised training', 'deep generative models', 'human visual cortex', 'parallelization algorithm', 'CIFAR-10', 'CIFAR-100', 'object recognition', 'unlabeled tiny images']\n",
      "['active learning', 'machine learning algorithm', 'labeled training instances', 'queries', 'unlabeled instances', 'oracle', 'human annotator', 'unlabeled data', 'labels', 'query strategy frameworks']\n",
      "['Ligninolytic enzymes', 'White rot fungi', 'Peroxidases', 'Lignin peroxidase', 'Manganese peroxidase', 'Laccase']\n",
      "已更新 61 个已处理的摘要标题。\n",
      "共现关系已保存到 'data1/1478_cooccurrence.csv'。\n",
      "处理完成。\n",
      "['FreeSel', 'data selection algorithm', 'informative samples', 'active learning methods', 'general-purpose models', 'semantic patterns', 'distance-based sampling', 'computer vision tasks', 'efficiency']\n",
      "未找到已处理摘要记录文件，初始化一个新的记录。\n",
      "未找到共现关系文件，初始化一个新的计数器。\n",
      "['multiclass online prediction', 'learnability', '$b$-ary Littlestone dimension', 'mistake trees', 'agnostic setting', 'negative regret', \"Littlestone's SOA\", \"Rosenblatt's Perceptron\", 'list online version', 'pattern classes']\n",
      "['PAC learnability', 'VC dimension', 'multiclass setting', 'DS dimension', 'list PAC learning', 'k predictions', 'multiclass learnability', 'k-list learn', 'k-DS dimension']\n",
      "['PAC learnability', 'Vapnik-Chervonenkis dimension', 'multiclass', 'DS dimension', 'list PAC learning', 'Natarajan dimension', 'concept classes', 'topology', 'colorful simplicial complexes', 'hyperbolicity']\n",
      "['conformal prediction', 'uncertainty quantification', 'distribution-free', 'neural network', 'machine learning tasks', 'structured outputs', 'distribution shift', 'time-series', 'code samples']\n",
      "['online boosting', 'regression tasks', 'limited information', 'regret minimization', 'online boosting algorithm', 'noisy multi-point bandit feedback', 'projection-free', 'online convex optimization', 'stochastic gradient', 'efficiency']\n",
      "['REBEL', 'multi-class boosting method', 'white-box learning systems', 'training error', 'exponential rate', 'overfitting', 'MNIST', 'UCI datasets']\n",
      "['ensemble learning algorithms', 'multi-class classification', 'deep decision trees', 'generalization guarantees', 'data-dependent learning bounds', 'Rademacher complexities', 'convex ensembles', 'mixture weight', 'H-consistency', 'AdaBoost']\n",
      "['multi-class boosting', 'classification', 'DMCBoost', 'base classifiers', 'ensemble classifier', 'non-convex performance measures', 'empirical classification error', 'margin functions', 'non-convex optimization', 'noisy cases']\n",
      "['Cost-sensitive multiclass classification', 'misclassification costs', 'classification algorithms', 'loss functions', 'guess-aversion', 'binary classification', 'multiclass boosting', 'GEL- and GLL-MCBoost', 'GLL loss function']\n",
      "['multiclass prediction', 'optimal learner', 'improper', 'hypothesis class', 'one-inclusion multiclass learner', 'sample complexity']\n",
      "['AdaBoost.MH', 'vector-valued decision trees', 'multi-class edge', 'vector-valued decision stump', 'input-independent vector', 'label-independent scalar classifier', 'binary classifier', 'multi-class boosting algorithm']\n",
      "['multi-class boosting', 'multi-dimensional codewords', 'margin enforcing loss', 'gradient descent', 'coordinate descent', 'CD-MCBoost', 'GD-MCBoost', 'Bayes consistent', 'convergent', 'AdaBoost']\n",
      "['boosting', 'weak classifiers', 'accurate predictors', 'binary classification', 'multiclass setting', 'requirements', 'optimal requirements', 'boosting algorithms', 'framework']\n",
      "['conformal prediction', 'error probability', 'on-line setting', 'support-vector machine', 'ridge regression', 'nearest-neighbor method', 'Gaussian linear model', 'theory', 'numerical examples', 'Algorithmic Learning in a Random World']\n",
      "['prediction model', 'i.i.d. sample', 'VC(Ƒ)', 'one-inclusion prediction strategy', 'one-inclusion graph', 'Peeling compression scheme', 'VC-invariant shifting', 'group-theoretic symmetrization', 'Pollard pseudo-dimension', 'hypergraph generalization']\n",
      "['multiclass categorization', 'binary problems', 'margin-based binary learning algorithm', 'error-correcting properties', 'empirical multiclass loss bound', 'support-vector machines', 'AdaBoost', 'regression', 'logistic regression', 'decision-tree algorithms']\n",
      "['algorithm', 'binary concepts', 'hypotheses', 'Schapire', 'polynomial PAC learning', 'threshold circuits', 'learnability', 'compression', 'parallelizing', 'distribution']\n",
      "['1)-valued functions', 'R/sup n/', \"L.G. Valiant's learnability model\", 'computational complexity', 'prediction strategies', '1-inclusion graph', 'geometric range queries', 'axis-parallel rectangles', 'halfspaces', 'optimal to within a constant factor']\n",
      "['boosting', 'multiclass boosting', 'weak hypotheses', 'weak learner', 'agnostic PAC learner', 'AdaBoost', 'number of classes', 'oracle calls', 'trade-off']\n",
      "['Machine learning', 'algorithmic paradigms', 'computational complexity', 'convexity', 'stability', 'stochastic gradient descent', 'neural networks', 'structured output learning', 'PACBayes approach', 'compression-based bounds']\n",
      "['learnability', 'two-valued functions', 'Data Compression', 'kernel', 'hypothesis', 'probability', 'random sample point', 'Vapnik-Chervonenkis dimension', 'machine learning']\n",
      "已更新 21 个已处理的摘要标题。\n",
      "共现关系已保存到 'data1/1479_cooccurrence.csv'。\n",
      "处理完成。\n",
      "['boosting', 'multiclass classification', 'weak learning', 'weak learnability', 'oracle complexity', 'List PAC Learning', 'weak PAC learning', 'list learners', 'error bound']\n",
      "未找到已处理摘要记录文件，初始化一个新的记录。\n",
      "未找到共现关系文件，初始化一个新的计数器。\n",
      "['natural signals', 'neural compression', 'discrete tokens', 'audio compression', 'high-fidelity', 'vector quantization', 'reconstruction losses', 'open-source code']\n",
      "['conditional music generation', 'MusicGen', 'Language Model', 'compressed discrete music representation', 'transformer', 'token interleaving patterns', 'textual description', 'melodic features', 'text-to-music benchmark', 'ablation studies']\n",
      "['Noise2Music', 'diffusion models', 'music clips', 'text prompts', 'generator model', 'cascader model', 'intermediate representation', 'high-fidelity audio', 'spectrogram']\n",
      "['SPEAR-TTS', 'multi-speaker', 'text-to-speech', 'minimal supervision', 'discrete speech representations', 'sequence-to-sequence tasks', 'audio-only data', 'pretraining', 'backtranslation', 'speaker identity']\n",
      "['MusicLM', 'high-fidelity music', 'text descriptions', 'sequence-to-sequence', '24 kHz', 'audio quality', 'melody', 'text caption', 'MusicCaps', 'music-text pairs']\n",
      "['diffusion models', 'high resolution images', 'latent diffusion', 'cascades', 'denoising diffusion', 'noise schedule', 'architecture', 'dropout', 'downsampling', 'ImageNet']\n",
      "['language modeling', 'text to speech', 'neural codec', 'Vall-E', 'conditional language modeling', 'pre-training', 'in-context learning', 'personalized speech', 'zero-shot TTS']\n",
      "['Generalized Speech Enhancement', 'audio-visual speech resynthesis', 'pseudo audio-visual speech recognition', 'pseudo text-to-speech synthesis', 'self-supervised speech model', 'in-the-wild video-to-speech synthesis', 'LRS3 audio-visual enhancement', 'EasyCom', 'ReVISE.']\n",
      "['deep generative models', 'audio synthesis', 'environmental sounds', 'autoregressive approach', 'diffusion-based generative model', 'waveform domain', 'quality and diversity', 'conditioning schemas']\n",
      "['neural networks', 'streaming encoder-decoder architecture', 'quantized latent space', 'multiscale spectrogram adversary', 'loss balancer mechanism', 'lightweight Transformer models', 'high-fidelity', 'real-time', 'audio codec', 'MUSHRA tests']\n",
      "['deep generative models', 'neural vocoders', 'singing voice', 'hierarchical diffusion model', 'multiple diffusion models', 'sampling rates', 'low-frequency components', 'pitch', 'waveform', 'acoustic features']\n",
      "['text-to-audio generation', 'auto-regressive generative model', 'discrete audio representation', 'augmentation technique', 'multi-stream modeling', 'classifier-free guidance', 'audio continuation']\n",
      "['diffusion-based generative models', 'design space', 'training process', 'preconditioning of the score networks', 'state-of-the-art FID', 'CIFAR-10', 'class-conditional setting', 'unconditional setting', 'ImageNet-64 model']\n",
      "['Imagen', 'text-to-image diffusion model', 'photorealism', 'large transformer language models', 'image synthesis', 'FID score', 'COCO dataset', 'DrawBench', 'image-text alignment', 'human raters']\n",
      "['CLIP', 'image generation', 'representations', 'image embedding', 'text caption', 'image diversity', 'diffusion models', 'decoder', 'language-guided image manipulations', 'zero-shot fashion']\n",
      "['Deep Noise Suppression', 'perceptual speech quality', 'subjective evaluation framework', 'ITU-T P.835', 'DNS-MOS', 'word accuracy', 'personalized noise suppression', 'fullband', 'WAcc', 'real-world scenarios']\n",
      "['lexical content', 'speaker identity', 'spoken language translation', 'phonetic-content units', 'prosodic features', 'neural vocoder', 'non-verbal vocalizations', 'model analysis']\n",
      "['S3PRL-VC', 'open-source', 'voice conversion', 'self-supervised speech representation', 'S3R', 'recognition-synthesis', 'VCC2020', 'intra-/cross-lingual', 'any-to-one (A2O)', 'any-to-any (A2A)']\n",
      "['Speech pre-training', 'classification tasks', 'GSLM', 'prosodic information', 'expressive speech', 'multi-stream transformer language model', 'prosodic feature streams', 'HiFi-GAN model', 'coherent speech']\n",
      "['SoundStream', 'neural audio codec', 'convolutional encoder/decoder', 'residual vector quantizer', 'end-to-end', 'structured dropout', 'variable bitrates', 'low latency', 'subjective evaluations', 'background noise suppression']\n",
      "['TTS', 'speech synthesis', 'neural network-based TTS', 'text analysis', 'acoustic models', 'vocoders', 'fast TTS', 'low-resource TTS', 'expressive TTS', 'adaptive TTS']\n",
      "['self-supervised', 'speech representation learning', 'sound units', 'Hidden-Unit BERT', 'HuBERT', 'clustering', 'BERT-like prediction loss', 'wav2vec 2.0', 'Librispeech', 'relative WER reduction']\n",
      "['cascaded diffusion models', 'high fidelity images', 'ImageNet', 'super-resolution', 'conditioning augmentation', 'FID scores', 'classification accuracy', 'BigGAN-deep', 'VQ-VAE-2']\n",
      "['diffusion models', 'image sample quality', 'unconditional image synthesis', 'classifier guidance', 'FID', 'ImageNet', 'BigGAN-deep', 'upsampling diffusion models', 'compute-efficient']\n",
      "['self-supervised', 'discrete representations', 'speech resynthesis', 'disentangled representation', 'speech content', 'prosodic information', 'speaker identity', 'reconstruction quality', 'ultra-lightweight speech codec', 'speech quality']\n",
      "['DDPM', 'log-likelihoods', 'sample quality', 'reverse diffusion process', 'sampling', 'precision and recall', 'GANs', 'target distribution', 'model capacity', 'training compute']\n",
      "['stochastic differential equation', 'reverse-time SDE', 'score-based generative modeling', 'neural networks', 'diffusion probabilistic modeling', 'predictor-corrector framework', 'neural ODE', 'exact likelihood computation', 'inverse problems', 'image generation']\n",
      "['GANs', 'speech synthesis', 'raw waveforms', 'HiFi-GAN', 'sinusoidal signals', 'periodic patterns', 'human evaluation', 'mean opinion score', 'high-fidelity audio', 'mel-spectrogram inversion']\n",
      "['AudioSet', 'FSD50K', 'sound event recognition', 'open dataset', 'audio clips', 'Creative Commons licenses', 'Freesound', 'AudioSet Ontology']\n",
      "['DiffWave', 'Diffusion probabilistic model', 'Waveform generation', 'non-autoregressive', 'neural vocoding', 'mel spectrogram', 'WaveNet vocoder', 'audio quality', 'sample diversity', 'unconditional generation']\n",
      "['WaveGrad', 'conditional model', 'waveform generation', 'gradients of the data density', 'score matching', 'diffusion probabilistic models', 'Gaussian white noise', 'gradient-based sampler', 'mel-spectrogram', 'inference speed']\n",
      "['diffusion probabilistic models', 'image synthesis', 'latent variable models', 'nonequilibrium thermodynamics', 'weighted variational bound', 'denoising score matching', 'Langevin dynamics', 'progressive lossy decompression', 'CIFAR10', '256x256 LSUN']\n",
      "['vector quantization', 'acoustic unit discovery', 'unlabelled data', 'discrete representations', 'VQ-VAE', 'contrastive predictive coding', 'ZeroSpeech 2020 challenge', 'ABX phone discrimination', 'voice conversion', 'speaker information']\n",
      "['audio-visual dataset', 'computer vision techniques', 'audio recognition models', 'scalable pipeline', 'YouTube', 'image classification', 'audio-visual correspondence', 'ambient noise', 'VGGSound', 'Convolutional Neural Network (CNN)']\n",
      "['ViSQOL', 'ViSQOLAudio', 'perceptual quality', 'audio', 'speech', 'C++ library', 'open source', 'production usage', 'Google', 'benchmarked']\n",
      "['Common Voice corpus', 'multilingual', 'transcribed speech', 'speech technology', 'Automatic Speech Recognition', 'crowdsourcing', 'data validation', 'audio corpus', 'transfer learning', 'Character Error Rate']\n",
      "['audio captioning', 'intermodal translation', 'audio signal', 'textual description', 'Clotho', 'dataset', 'Freesound platform', 'Amazon Mechanical Turk']\n",
      "['GANs', 'coherent waveforms', 'high quality mel-spectrogram inversion', 'speech synthesis', 'music domain translation', 'unconditional music synthesis', 'non-autoregressive', 'fully convolutional', 'ablation studies', 'pytorch implementation']\n",
      "['source separation', 'music', 'Conv-Tasnet', 'waveform-to-waveform', 'MusDB dataset', 'masking approach', 'Demucs', 'signal to distortion ratio', 'mean opinion score']\n",
      "['Audio Captioning', 'natural language description', 'audio in the wild', 'large-scale dataset', 'human-written text', 'crowdsourcing', 'AudioSet', 'audio representation', 'captioning models', 'audio captioning performance']\n",
      "['ASR-TTS autoencoder', 'unsupervised', 'discrete subword units', 'Multilabel-Binary Vectors', 'voice conversion', 'TTS-Decoder', 'adversarial training', 'speaker identity', 'ZeroSpeech 2019 Challenge', 'low bitrate']\n",
      "['supervised learning', 'unsupervised learning', 'Contrastive Predictive Coding', 'representations', 'high-dimensional data', 'autoregressive models', 'probabilistic contrastive loss', 'negative sampling', 'modality', 'reinforcement learning']\n",
      "['Audio event recognition', 'machine perception', 'comprehensive datasets', 'Audio Set', 'manually-annotated audio events', 'hierarchical ontology', 'audio classes', 'human labelers', 'YouTube videos', 'content analysis']\n",
      "['WaveNet', 'deep neural network', 'raw audio waveforms', 'probabilistic', 'autoregressive', 'text-to-speech', 'state-of-the-art', 'speakers', 'music', 'phoneme recognition']\n",
      "['Opus codec', 'audio transmission', 'Internet']\n",
      "['Denoising diffusion probabilistic models', 'gradient of the data density', 'Gaussian distribution', 'adaptive prior', 'conditional diffusion model', 'vocoder', 'mel-spectrogram', 'audio generative models', 'spectral and time domains', 'data-driven adaptive prior']\n",
      "['ML4MD', 'Machine Learning', 'Music Discovery', 'ICML2019', 'Long Beach', 'California', '15 de juny de 2019']\n",
      "['intermediate audio quality', 'subjective assessment', 'Recommendation ITU-R BS.1116', 'grading scale']\n",
      "已更新 48 个已处理的摘要标题。\n",
      "共现关系已保存到 'data1/1480_cooccurrence.csv'。\n",
      "处理完成。\n",
      "['Deep generative models', 'audio waveforms', 'diffusion models', 'speech vocoders', 'high-fidelity', 'multi-band', 'audio modality', 'low-bitrate', 'perceptual quality', 'audiocraft Github page.']\n",
      "未找到已处理摘要记录文件，初始化一个新的记录。\n",
      "未找到共现关系文件，初始化一个新的计数器。\n",
      "['dynamic graph learning', 'neighbor co-occurrence encoding', 'patching technique', 'DyGLib', 'dynamic link prediction', 'dynamic node classification', 'state-of-the-art performance', 'temporal dependencies']\n",
      "['RNN', 'self-attention mechanism', 'temporal graph learning', 'GraphMixer', 'multi-layer perceptrons', 'link-encoder', 'node-encoder', 'mean-pooling', 'link prediction', 'generalization performance']\n",
      "['temporal networks', 'machine learning', 'line graph', 'edge classification', 'edge representations', 'synthetic model', 'real-world networks', 'temporal link prediction']\n",
      "['Graph Neural Networks', 'Link Prediction', 'expressive power', 'subgraph-based methods', 'structural features', 'subgraph GNN', 'ELPH', 'Message Passing GNNs', 'BUDDY']\n",
      "['Temporal graph networks', 'embedding dynamic interactions', 'representational power', 'temporal walks', 'message passing', 'recurrent memory', 'injective updates', 'injective temporal message passing', 'positional features']\n",
      "['spatiotemporal time series', 'forecasting architecture', 'computational complexity', 'scalable architecture', 'randomized recurrent neural network', 'high-dimensional state representations', 'unsupervised', 'multi-scale spatiotemporal representations']\n",
      "['temporal networks', 'real-world complex systems', 'representation learning', 'neighborhood representation', 'structural features', 'joint neighborhood', 'N-cache', 'GPUs', 'link prediction accuracy', 'scalable']\n",
      "['Graph Neural Networks', 'dynamic graphs', 'ROLAND', 'graph representation learning', 'node embeddings', 'live-update evaluation', 'incremental training', 'meta-learning', 'future link prediction', 'mean reciprocal rank']\n",
      "['dynamic graphs', 'link prediction', 'evaluation procedures', 'visualization techniques', 'EdgeBank', 'memorization baseline', 'negative sampling strategies', 'real-world applications', 'dynamic graph datasets']\n",
      "['Graph Anomaly Detection', 'DGraph', 'dynamic graph', 'finance domain', 'anomalous nodes', 'normal nodes', 'fraudsters']\n",
      "['Graph Transformers', 'GPS', 'graph representation learning', 'structural encoding', 'local message-passing', 'global attention', 'GraphGPS', 'linear complexity', 'benchmarks']\n",
      "['user-created content', 'social interaction', 'user activity patterns', 'durable user lifespan', 'Reddit user interactions', 'content creation activity', 'perceived quality', 'content categories', 'user experience', 'feeling of belonging']\n",
      "['Temporal Graph Neural Networks', 'dynamic node embeddings', 'Temporal-CSR data structure', 'parallel sampler', 'random chunk scheduling', 'real-world datasets', 'multi-core CPU', 'link prediction', 'node classification']\n",
      "['Subgraph-based graph representation learning', 'scalability', 'SGRL', 'canonical graph neural networks', 'SUREL', 'walk-based decomposition', 'parallel computation', 'prediction performance']\n",
      "['Knowledge graph embedding', 'KGE', 'distance-based methods', 'link prediction', 'entity representations', 'head entity', 'tail entity', 'InterHT', 'InterHT+', 'ogbl-wikikg2 dataset']\n",
      "['hyperbolic temporal graph network', 'temporal networks', 'hyperbolic geometry', 'hierarchical information', 'hyperbolic graph neural network', 'hyperbolic gated recurrent neural network', 'hyperbolic temporal contextual self-attention', 'hyperbolic temporal consistency', 'temporal link prediction', 'AUC improvement']\n",
      "['Transformer architecture', 'Spectral Attention Network', 'learned positional encoding', 'Laplacian spectrum', 'fully-connected Transformer', 'over-squashing', 'GNNs', 'graph benchmarks', 'attention-based model']\n",
      "['dynamic graph modeling', 'continuous-time', 'graph neural network', 'Transformer', 'temporal', 'topology', 'two-stream encoder', 'co-attentional transformer', 'contrastive learning', 'mutual information']\n",
      "['machine learning', 'large-scale graph data', 'graph ML', 'OGB Large-Scale Challenge', 'datasets', 'link prediction', 'graph regression', 'node classification', 'baseline experiments', 'KDD Cup']\n",
      "['temporal networks', 'triadic closure', 'Causal Anonymous Walks', 'temporal random walks', 'network motifs', 'anonymization strategy', 'neural-network model', 'inductive setting']\n",
      "['transformer neural network architecture', 'arbitrary graphs', 'attention mechanism', 'neighborhood connectivity', 'positional encoding', 'Laplacian eigenvectors', 'batch normalization', 'edge feature representation', 'graph benchmark', 'graph neural networks']\n",
      "['recommender systems', 'information overload', 'user/item representations', 'graph neural network', 'GNN', 'graph structure', 'graph representation learning', 'GNN-based recommender systems', 'recommendation models', 'open-source implementations']\n",
      "['OpenSky Network', 'global collection', 'air traffic control data', 'researchers', 'COVID-19 outbreak', 'aircraft flight data', 'comprehensive air traffic dataset', 'flights', 'airports', 'countries']\n",
      "['Graph Neural Networks', 'dynamic graphs', 'Temporal Graph Networks', 'memory modules', 'graph-based operators', 'timed events', 'deep learning', 'state-of-the-art performance', 'prediction tasks', 'ablation study']\n",
      "['Open Graph Benchmark', 'benchmark datasets', 'graph machine learning', 'large-scale graphs', 'multiple important graph ML tasks', 'diverse range of domains', 'unified evaluation protocol', 'scalability', 'out-of-distribution generalization', 'automated end-to-end graph ML pipeline']\n",
      "['temporal graphs', 'temporal graph attention', 'TGAT', 'self-attention', 'time encoding', 'node embeddings', 'link prediction', 'temporal edge features', 'dynamic networks']\n",
      "['novel coronavirus', 'Wuhan', 'China', 'international disease transmission']\n",
      "['generating reviews', 'recommendation justification', 'personalized recommendation', 'Seq2Seq model', 'aspect-planning', 'masked language model', 'diverse justifications', 'experiments', 'real-world datasets']\n",
      "['dynamic graphs', 'hierarchical variational model', 'graph recurrent neural network', 'latent random variables', 'node attribute changes', 'semi-implicit variational inference', 'non-Gaussian latent representations', 'dynamic graph analytic tasks', 'dynamic link prediction']\n",
      "['Modeling sequential interactions', 'dynamic embedding', 'embedding trajectory', 'recurrent neural network', 'projection operator', 'future user-item interactions', 't-Batch algorithm', 'prediction tasks', 'real-world datasets']\n",
      "['Graph neural networks', 'representation power', 'universal approximation', 'permutation-invariant functions', 'graph isomorphism tests', 'sigma-algebra', 'expressive power', 'order-2 Graph G-invariant networks', 'Ring-GNNs', 'social network datasets']\n",
      "['Graphs', 'machine learning', 'static graphs', 'evolving graphs', 'representation learning', 'dynamic graphs', 'dynamic knowledge graphs', 'encoder-decoder', 'applications', 'datasets']\n",
      "['Graph representation learning', 'graph neural networks', 'non-Euclidean domain', 'node embeddings', 'recurrent neural network', 'temporal dynamics', 'graph convolutional network', 'parameter evolution', 'link prediction', 'node classification']\n",
      "['Artificial Intelligence', 'machine learning', 'transformation', 'health systems', 'efficiency', 'effectiveness', 'universal health coverage', 'improve outcomes']\n",
      "['graph attention networks', 'neural network architectures', 'graph-structured data', 'masked self-attentional layers', 'spectral-based graph neural networks', 'transductive problems', 'Cora', 'Citeseer', 'Pubmed']\n",
      "['graph-structured data', 'semi-supervised learning', 'convolutional neural networks', 'localized first-order approximation', 'spectral graph convolutions', 'scalable', 'hidden layer representations', 'local graph structure', 'citation networks', 'knowledge graph dataset']\n",
      "['global food security', 'resource sustainability', 'irrigation water', 'traded animal products', 'cropland', 'wheat', 'soybean', 'maize']\n",
      "['network science', 'journal', 'field in formation', 'scope', 'contents', 'foundations', 'vision', 'emerging science', 'networks']\n",
      "['networks', 'taxonomies', 'structural similarities', 'community structures', 'empirical', 'synthetic', 'political voting data', 'financial data', 'Facebook networks', 'fungi']\n",
      "['Internet', 'ImageNet', 'ontology', 'WordNet', 'database', 'Amazon Mechanical Turk', 'object recognition', 'image classification', 'computer vision']\n",
      "['network motifs', 'complex networks', 'randomized networks', 'biochemistry', 'neurobiology', 'ecology', 'engineering', 'information processing', 'Caenorhabditis elegans', 'World Wide Web']\n",
      "['retrieval environments', 'relevant documents', 'IR techniques', 'evaluation approaches', 'graded relevance judgments', 'cumulative gain', 'discount factor', 'TREC data', 'four-point scale', 'statistical significance']\n",
      "['continuous-time dynamic graphs', 'representation learning', 'Neural Temporal Walks', 'spatiotemporal-biased random walks', 'temporal nodes', 'neural ordinary differential equations', 'irregularly-sampled', 'spatiotemporal dynamics', 'contrastive pretext task']\n",
      "['Graph neural networks', 'link prediction', 'DGNNs', 'heuristics', 'sliding window sizes', 'static GNNs', 'dynamic networks', 'empirical comparison']\n",
      "['machine learning', 'blockchain graphs', 'ransomware payment tracking', 'price manipulation analysis', 'money laundering detection', 'Chartalist', 'UTXO', 'Bitcoin', 'Ethereum', 'open-science']\n",
      "['temporal graphs', 'equivariant representations', 'node attribute evolution', 'time-and-graph', 'time-then-graph', 'GNN', 'RNN', 'expressivity advantage', '1-Weisfeiler-Lehman GNNs', 'graph ML toolbox']\n",
      "['Million Song Dataset', 'audio features', 'metadata', 'contemporary popular music tracks', 'creation process', 'possible uses', 'existing resources', 'largest current research dataset', 'year prediction', 'future development']\n",
      "已更新 47 个已处理的摘要标题。\n",
      "共现关系已保存到 'data1/1481_cooccurrence.csv'。\n",
      "处理完成。\n",
      "['Temporal Graph Benchmark', 'benchmark datasets', 'machine learning models', 'temporal graphs', 'node and edge-level prediction tasks', 'dynamic node property prediction', 'temporal graph models', 'automated machine learning pipeline', 'reproducible and accessible']\n",
      "未找到已处理摘要记录文件，初始化一个新的记录。\n",
      "未找到共现关系文件，初始化一个新的计数器。\n",
      "['Rasch model', 'item response theory', 'item estimation', 'binary response', 'Markov chain', 'finite-sample error guarantees', 'consistent', 'optimality properties', 'real-life datasets', 'recommendation systems']\n",
      "['data-efficient training', 'AdaCore', 'geometry of the data', 'curvature of the loss function', 'Hessian', 'logistic regression', 'neural networks']\n",
      "['Computerized Adaptive Testing', 'personalized online education', 'selection algorithm', 'proficiency estimation', 'multi-facet', 'Robust Adaptive Testing', 'statistical properties', 'synthetic data', 'real-world datasets']\n",
      "['Computerized Adaptive Testing', 'personalized test', 'informativeness/uncertainty metrics', 'Neural Computerized Adaptive Testing', 'reinforcement learning', 'bilevel optimization', 'neural policy', 'real-world datasets', 'effectiveness and robustness']\n",
      "['Cognitive diagnosis models', 'educational assessment', 'individualized feedback', 'biases', 'aberrant response patterns', 'person-fit method', 'machine learning', 'simulations', 'cheating', 'short-length tests.']\n",
      "['Cognitive diagnosis', 'educational measurement', 'psychology', 'learning states', 'knowledge concepts', 'artificial neural network', 'non-linear interactions', 'graph structure', 'evaluation', 'accuracy']\n",
      "['Computerized adaptive testing', 'CAT', 'item response theory', 'IRT', 'student ability', 'question selection algorithms', 'BOBCAT', 'Bilevel Optimization-Based framework', 'adaptive testing', 'student response datasets']\n",
      "['Cognitive diagnosis', 'intelligent educational settings', 'student-exercise interactions', 'inter-layer interaction modeling', 'structural relations', 'educational interdependencies', 'attention network', 'diagnosis function', 'real-world datasets']\n",
      "['robust coreset', 'continuous-and-bounded learning', 'outliers', 'fully-dynamic environment', 'optimization objectives', 'logistic regression', 'doubling dimension', 'real-world datasets', 'adversarial attacker']\n",
      "['optimization', 'machine learning', 'clustering problems', 'metric space', 'objective function', 'coreset', 'streaming', 'distributed', 'dynamic data', 'approximation error']\n",
      "['Computerized Adaptive Testing', 'cognitive model', 'Model-Agnostic Adaptive Testing', 'quality', 'diversity', 'Active Learning', 'quality and diversity', 'submodular property', 'experimental results', 'datasets']\n",
      "['active observations', 'active surveillance', 'spatio-temporal diffusion system', 'key components', 'sentinel network', 'row sparsity structure', 'backward-selection', 'sentinel network mining algorithm', 'group sparse Bayesian learning', 'non-linear dynamical system']\n",
      "['Digital technologies', 'education', 'personalized', 'diagnostic questions', 'misconceptions', 'learning curriculum recommendations', 'multiple-choice', 'Eedi', 'educational platform']\n",
      "['adaptive learning problem', 'individualized learning plan', 'continuous latent traits', 'Markov decision process', 'model-free deep reinforcement learning algorithm', 'deep Q-learning algorithm', 'transition model estimator', 'neural networks', 'optimal learning policy']\n",
      "['text classification', 'active learning', 'annotation cost', 'informative examples', 'classification model', 'unlabeled data', 'sparse reconstruction', 'summary words', 'annotation cost', 'classification performance']\n",
      "['Cognitive diagnosis', 'intelligent education', 'neural networks', 'exercising interactions', 'factor vectors', 'neural layers', 'monotonicity assumption', 'interpretability', 'Q-matrix']\n",
      "['student performance prediction', 'proactive services', 'exercising records', 'knowledge acquisition', 'Exercise-Enhanced Recurrent Neural Network', 'bidirectional LSTM', 'Attention mechanism', 'Exercise-aware Knowledge Tracing', 'knowledge concepts', 'memory network']\n",
      "['deletion-robust submodular maximization', 'privacy', 'fairness', 'memory-efficient', 'centralized', 'streaming', 'distributed', 'adversarial deletions', 'approximation guarantees', 'summarization']\n",
      "['maximum model change', 'active learning', 'classification', 'model change', 'stochastic gradient', 'support vector machines', 'logistic regression', 'uncertainty-based sampling', 'UCI repository', 'ImageNet']\n",
      "['Machine learning', 'personalized learning systems', 'SPARse Factor Analysis', 'learning analytics', 'content analytics', 'learner concept knowledge profiles', 'question difficulties', 'ordinal scale', 'Ordinal SPARFA-Tag', 'educational data']\n",
      "['social networks', 'viral marketing', 'social influence', 'influence propagation', 'influence maximization', 'Group-PageRank', 'algorithms', 'linear approach', 'social influence models']\n",
      "['empirical risk minimization', 'machine learning', 'data mining', 'active learning', 'source distribution', 'true risk', 'upper bound', 'batch mode active learning', 'non-convex integer programming', 'alternating optimization method']\n",
      "['student affect', 'web-based tutoring platform', 'learning outcomes', 'tutor log-data', 'student affective states', 'boredom', 'engaged concentration', 'confusion', 'frustration']\n",
      "['Non-negative Matrix Factorization', 'student skills', 'Q-matrix', 'assessment', 'skill mastery', 'simulated data', 'variance', 'skill effect', 'analysis and visualization techniques']\n",
      "['algorithm', 'stochastic gradient descent', 'convergence', 'matrix factorization', 'distributed', 'MapReduce', 'scalability']\n",
      "['active learning', 'labeling cost', 'informative', 'representative', 'query selection', 'ad hoc', 'min-max view', 'QUIRE', 'multi-label learning', 'experimental results']\n",
      "['Item Response Theory', 'survey instruments', 'mathematical models', 'measuring abilities', 'attitudes', 'statistical analysis', 'high stakes tests', 'Graduate Record Examination', 'methods sections', 'journal articles']\n",
      "['feature selection', 'k-means clustering', 'accuracy parameter', 'unsupervised', 'arbitrary dimensions', 'γ-approximate k-means', 'partition', 'high probability']\n",
      "['unsupervised extractive summarization', 'semantic graph', 'submodular functions', 'near-optimal', 'ICSI meeting summarization', 'human transcripts', 'automatic speech recognition', 'graph-based submodular selection', 'maximum marginal relevance', 'PageRank']\n",
      "['outbreak detection', 'sensor placement', 'water distribution network', 'submodularity', 'efficient algorithm', 'real-world problems', 'detection likelihood', 'blog data', 'sensor locations', 'optimal solution']\n",
      "['k-median', 'k-means clustering', 'low dimension', 'point set', 'Rd', 'weighted set', '(1+ε)-approximation', 'linear running time', 'polylogarithmic space', 'update time']\n",
      "['computerized adaptive testing', 'Fisher information', 'item information', 'trait level', 'global information', 'item selection', 'simulation studies', 'bias', 'mean squared error', 'Kullback-Leibler information']\n",
      "['alternative item selection criteria', 'adaptive testing', 'uncertainty', 'ability estimates', 'weighted information criterion', 'maximum information criterion', 'simulation study', 'likelihood weighted information criterion', 'maximum likelihood estimator', 'Bayesian expected a posteriori estimator']\n",
      "['recurrent neural networks', 'input sequences', 'output sequences', 'recognition', 'production', 'prediction', 'training', 'gradient based learning algorithms', 'temporal contingencies', 'efficient learning']\n",
      "['Vector Analysis', 'Curved Coordinates', 'Tensors', 'Determinants', 'Matrices', 'Group Theory', 'Special Functions', 'Fourier Series', 'Integral Equations', 'Chaos']\n",
      "['user modeling', 'latent characteristics', 'decision-making', 'biases', 'sensitive attributes', 'fairness', 'FairLISA framework', 'adversarial framework', 'recommender system', 'cognitive diagnosis']\n",
      "['latent models', 'recommender systems', 'cold start', 'neural network', 'DropoutNet', 'user-item interactions', 'optimization', 'dropout', 'accuracy']\n",
      "['tailored testing', 'sequential testing', 'classification decision', 'estimated classification probabilities', 'terminate testing']\n",
      "['active learning', 'machine learning', 'labeled training instances', 'unlabeled instances', 'oracle', 'human annotator', 'unlabeled data', 'query strategy frameworks', 'theoretical evidence']\n",
      "已更新 39 个已处理的摘要标题。\n",
      "共现关系已保存到 'data1/1482_cooccurrence.csv'。\n",
      "处理完成。\n",
      "['Computerized adaptive testing', 'ability estimation accuracy', 'student’s true ability', 'statistical properties', 'Bounded Ability Estimation framework', 'data-summary manner', 'question subset', 'gradient of the full responses', 'greedy selection algorithm', 'error upper-bound guarantees']\n",
      "未找到已处理摘要记录文件，初始化一个新的记录。\n",
      "未找到共现关系文件，初始化一个新的计数器。\n",
      "['agent', 'uncertainty estimation', 'ensemble of models', 'training ensembles', 'prior functions', 'bootstrapping', 'joint predictions', 'signal-to-noise ratio', 'experimental results']\n",
      "['marginal predictions', 'joint predictive distributions', 'decision problems', 'low-order predictive distributions', 'high-dimensional inputs', 'dyadic sampling', 'logistic regression']\n",
      "['Neural Testbed', 'open-source benchmark', 'joint predictions', 'marginal predictions', 'Bayesian deep learning', 'neural network data generating process', 'downstream decision tasks', 'practical importance']\n",
      "['prediction', 'intelligent system', 'supervised learning', 'marginal predictions', 'joint predictions', 'decision problems', 'combinatorial decision problems', 'sequential predictions', 'multi-armed bandits', 'regret bound']\n",
      "['deep learning', 'uncertainty', 'robustness', 'estimates', 'methods', 'tasks', 'metrics', 'reproducibility', 'baselines']\n",
      "['image classification', 'noisy labels', 'probabilistic approach', 'heteroscedastic', 'neural network', 'covariance matrix', 'aleatoric uncertainty', 'Imagenet ILSVRC 2012', 'WebVision 1.0', 'deep classifier']\n",
      "['Bayesian neural networks', 'high-dimensional', 'non-convex', 'full-batch Hamiltonian Monte Carlo', 'posterior tempering', 'cold posterior', 'data augmentation', 'prior scale', 'domain shift', 'predictive distributions']\n",
      "['Reinforcement learning', 'data efficiency', 'simulated environments', 'real environments', 'information acquisition', 'representation', 'regret analysis', 'data-efficient agents']\n",
      "['Inducing point Gaussian process approximations', 'uncertainty estimation', 'high dimensional inputs', 'Deep Kernel Learning', 'unreliable uncertainty estimates', 'feature extractor', 'bi-Lipschitz constraint', 'DUE', 'neural networks.']\n",
      "['ensembles', 'cascades', 'committee-based models', 'deep learning', 'efficiency', 'image classification', 'video classification', 'semantic segmentation', 'EfficientNet', 'ViT']\n",
      "['uncertainty estimation', 'deep learning', 'predictive correlations', 'Bayesian models', 'transductive active learning', 'TAL', 'meta-correlations', 'cross-normalized likelihoods', 'Bayesian neural net', 'Gaussian process models']\n",
      "['ensemble neural networks', 'robustness', 'uncertainty performance', 'forward passes', 'computational cost', 'multi-input multi-output (MIMO)', 'subnetworks', 'negative log-likelihood', 'accuracy', 'calibration error']\n",
      "['deep ensembles', 'Gaussian processes', 'Neural Tangent Kernel', 'infinite width limit', 'squared error loss', 'posterior interpretation', 'Bayesian deep ensembles', 'posterior predictive distribution', 'out-of-distribution settings', 'regression and classification tasks']\n",
      "['Bayesian neural networks', 'deep ensembles', 'predictive uncertainty', 'deep learning', 'high-quality uncertainty estimation', 'minimax learning problem', 'input distance awareness', 'Spectral-normalized Neural Gaussian Process', 'weight normalization', 'Gaussian process']\n",
      "['Deep reinforcement learning', 'RL algorithms', 'Acme', 'agent implementations', 'reproduce', 'extend', 'baseline implementations']\n",
      "['epistemic uncertainty', 'dropout', 'ensemble methods', 'neural network', 'Normalizing Flows', 'posterior distribution', 'OOD detection', 'uncertainty calibration', 'dataset shifts']\n",
      "['pre-training', 'fine-tuning', 'task-agnostic', 'few-shot performance', 'GPT-3', 'autoregressive language model', '175 billion parameters', 'translation', 'question-answering', 'societal impacts']\n",
      "['hypermodels', 'epistemic uncertainty', 'exploration', 'ensembles', 'Thompson sampling', 'computational cost', 'information-directed sampling', 'neural network hypermodels', 'hypernetworks']\n",
      "['probabilistic method', 'generative process', 'gradient based training', 'softmax temperature', 'bias-variance trade-off', 'image classification benchmarks', 'Imagenet-21k', 'image segmentation']\n",
      "['marginalization', 'Bayesian inference', 'deep neural networks', 'calibration', 'accuracy', 'structured prior', 'inductive biases', 'generalization', 'Bayesian model average']\n",
      "['empirical scaling laws', 'language model performance', 'cross-entropy loss', 'model size', 'dataset size', 'amount of compute', 'overfitting', 'training speed', 'sample-efficient', 'compute-efficient training']\n",
      "['Neural Tangents', 'infinite-width neural networks', 'Neural Network Gaussian Process', 'Neural Tangent kernels', 'gradient descent', 'CPU', 'GPU', 'TPU', 'distributed', 'Colab notebook']\n",
      "['deep ensembles', 'Bayesian neural networks', 'uncertainty', 'out-of-distribution robustness', 'random initialization', 'dataset shift', 'loss landscape', 'optimization trajectory', 'subspace sampling methods']\n",
      "['bsuite', 'reinforcement learning', 'experiments', 'agents', 'learning algorithms', 'benchmarks', 'evaluation', 'analysis', 'Python']\n",
      "['predictive uncertainty', 'supervised learning', 'deep learning', 'dataset shift', 'calibration', 'probabilistic deep learning', 'Bayesian methods', 'large-scale benchmark', 'classification problems', 'marginalize over models']\n",
      "['ImageNet-C', 'corruption robustness', 'classifiers', 'ImageNet-P', 'perturbations', 'adversarial defense', 'perturbation robustness', 'networks', 'generalize']\n",
      "['Variational Bayesian neural networks', 'variational inference', 'functional variational Bayesian neural networks', 'stochastic processes', 'Evidence Lower BOund', 'KL divergence', 'stochastic processes', 'Gaussian processes', 'implicit stochastic processes', 'reliable uncertainty estimates']\n",
      "['machine learning models', 'documentation', 'performance characteristics', 'model cards', 'transparent model reporting', 'benchmarked evaluation', 'intended application domains', 'responsible democratization']\n",
      "['uncertainty estimation', 'reinforcement learning', 'deep learning', 'sequential decision problems', 'bootstrap sampling', \"randomized untrainable `prior' network\", 'ensemble member', 'linear representations', 'nonlinear representations', 'large-scale problems']\n",
      "['Machine learning', 'experimental work', 'test sets', 'overfitting', 'accuracy', 'CIFAR-10 classifiers', 'unseen images', 'data distribution', 'deep learning models']\n",
      "['Uncertainty', 'predictive uncertainty', 'distributional mismatch', 'Prior Networks', 'distributional uncertainty', 'classification', 'misclassification', 'MNIST', 'CIFAR-10']\n",
      "['deep neural networks', 'theoretical properties', 'Gaussian processes', 'recursive kernel definition', 'random function', 'distribution', 'maximum mean discrepancy', 'Bayesian deep networks', 'predictive quantities', 'non-Gaussian alternative models']\n",
      "['Gaussian multiplicative noise', 'stochastic regularisation', 'deterministic neural networks', 'Bayesian neural networks', 'log-uniform prior', 'Bayesian inference', 'correlated weight noise', 'overfitting', 'minimum description length', 'additive reparametrisation']\n",
      "['neural network', 'Gaussian process', 'Bayesian inference', 'infinite width', 'covariance function', 'MNIST', 'CIFAR-10', 'prediction error', 'signal propagation', 'random neural networks']\n",
      "['Thompson sampling', 'online decision problems', 'exploiting', 'investing', 'computationally efficient', 'Bernoulli bandit problems', 'product recommendation', 'reinforcement learning', 'Markov decision processes', 'alternative algorithms']\n",
      "['Transformer', 'attention mechanisms', 'sequence transduction models', 'encoder-decoder', 'machine translation', 'parallelizable', 'BLEU score', 'WMT 2014', 'English-to-German', 'English-to-French']\n",
      "['Thompson sampling', 'online decision problems', 'posterior distribution', 'ensemble sampling', 'neural networks', 'tractability', 'theoretical basis', 'computational results']\n",
      "['constant SGD', 'stationary distribution', 'Bayesian posterior inference', 'Kullback-Leibler divergence', 'variational EM algorithm', 'SGD with momentum', 'Langevin Dynamics', 'Stochastic Gradient Fisher Scoring', 'approximation errors', 'Polyak averaging']\n",
      "['randomized value functions', 'deep exploration', 'reinforcement learning', 'value function learning', 'exploration', 'computational studies', 'regret bound', 'statistical efficiency', 'tabular representation']\n",
      "['aleatoric uncertainty', 'epistemic uncertainty', 'Bayesian deep learning', 'computer vision', 'input-dependent aleatoric uncertainty', 'semantic segmentation', 'depth regression', 'uncertainty formulation', 'loss functions', 'noisy data']\n",
      "['active learning', 'deep learning', 'Bayesian deep learning', 'Bayesian convolutional neural networks', 'model uncertainty', 'high dimensional data', 'image data', 'MNIST dataset', 'skin cancer diagnosis', 'ISIC2016 task']\n",
      "['multiplicative noise', 'neural networks', 'auxiliary random variables', 'variational setting', 'Bayesian neural networks', 'normalizing flows', 'local reparametrizations', 'tractable lower bound', 'predictive accuracy', 'predictive uncertainty']\n",
      "['Deep neural networks', 'predictive uncertainty', 'Bayesian NNs', 'distribution over weights', 'computationally expensive', 'high quality predictive uncertainty estimates', 'classification and regression benchmarks', 'robustness to dataset shift', 'out-of-distribution examples', 'ImageNet.']\n",
      "['deep artificial neural networks', 'generalization error', 'model family', 'regularization techniques', 'systematic experiments', 'convolutional networks', 'stochastic gradient methods', 'random labeling', 'expressivity', 'traditional models']\n",
      "['artificial intelligence', 'perception', 'deep learning', 'probabilistic graphical models', 'Bayesian nature', 'Bayesian deep learning', 'probabilistic framework', 'inference', 'text or images', 'neural networks']\n",
      "['bootstrapped DQN', 'exploration', 'reinforcement learning', 'randomized value functions', 'epsilon-greedy exploration', 'learning times', 'performance', 'Atari games', 'stochastic MDPs']\n",
      "['residual learning framework', 'residual networks', 'optimize', 'ImageNet', 'error', 'ILSVRC 2015', 'CIFAR-10', 'depth', 'COCO object detection dataset']\n",
      "['adaptive analysis', 'bias', 'false discoveries', 'reproducibility', 'mutual information', 'feature selection', 'variance-based selection', 'random noise', 'differential privacy', 'blinded data analysis']\n",
      "['Bayes by Backprop', 'backpropagation-compatible', 'probability distribution', 'neural network', 'variational free energy', 'marginal likelihood', 'regularisation', 'MNIST classification', 'non-linear regression', 'reinforcement learning']\n",
      "['exploration', 'Thompson sampling', 'posterior distributions', 'bootstrap technique', 'artificially generated data', 'prior distribution', 'multi-armed bandit', 'reinforcement learning', 'deep learning']\n",
      "['deep learning', 'model uncertainty', 'Bayesian models', 'dropout training', 'deep neural networks', 'approximate Bayesian inference', 'deep Gaussian processes', 'regression and classification', 'MNIST', 'deep reinforcement learning']\n",
      "['Bayes by Backprop', 'backpropagation-compatible', 'probability distribution', 'weights', 'neural network', 'variational free energy', 'marginal likelihood', 'dropout', 'MNIST classification']\n",
      "['ensemble of models', 'single model', 'compression technique', 'MNIST', 'acoustic model', 'commercial system', 'specialist models', 'mixture of experts']\n",
      "['algorithm', 'first-order gradient-based optimization', 'stochastic objective functions', 'adaptive estimates', 'non-stationary objectives', 'sparse gradients', 'hyper-parameters', 'theoretical convergence properties']\n",
      "['randomized least-squares value iteration', 'reinforcement learning', 'exploration', 'generalization', 'linearly parameterized value functions', 'least-squares value iteration', 'Boltzmann', 'epsilon-greedy', 'efficiency', 'expected regret']\n",
      "['convolutional neural network', 'ImageNet', 'LSVRC-2010', 'top-5 error rate', '60 million parameters', 'dropout', 'GPU implementation', 'ILSVRC-2012']\n",
      "['iterative learning', 'large scale datasets', 'mini-batches', 'stochastic gradient optimization', 'posterior distribution', 'anneal the stepsize', 'Bayesian posterior sampling', 'Monte Carlo estimates', 'mixture of Gaussians', 'logistic regression']\n",
      "['gradient descent', 'random initialization', 'non-linear activations', 'logistic sigmoid activation', 'saturated units', 'activation function', 'unsupervised pretraining', 'supervised training']\n",
      "['Internet', 'ImageNet', 'WordNet', 'ontology of images', 'semantic hierarchy', 'Amazon Mechanical Turk', 'object recognition', 'image classification', 'automatic object clustering']\n",
      "['information deviation', 'finite measures', 'statistical operations', 'Markov morphisms', 'morphism', 'sufficient']\n",
      "['Supervised neural networks', 'generalize', 'weights', 'penalizing', 'Gaussian noise', 'expected squared error', 'derivatives', 'non-linear hidden units']\n",
      "['Bayesian framework', 'feedforward networks', 'network architectures', 'network pruning', 'weight decay', 'regularizers', 'effective number of well-determined parameters', 'error bars', \"Occam's razor\", 'generalization ability']\n",
      "['Auditory Cortex', 'Probability Model', 'Communication']\n",
      "['statistical decision functions', 'general theory', 'classical non-sequential case', 'sequential case', 'assumptions', 'statistical problems', 'weaker set of conditions', 'methods of proofs', 'basic definitions']\n",
      "['Bayesian inference', 'function space', 'Functional Variational Inference', 'divergence', 'variational distribution', 'posterior process', 'Stochastic Process Generators', 'functional ELBO', 'stochastic process priors', 'mini-batch sampling']\n",
      "['BERT', 'Bidirectional Encoder Representations from Transformers', 'pre-train', 'unlabeled text', 'state-of-the-art', 'natural language processing', 'GLUE score', 'MultiNLI accuracy', 'SQuAD v1.1', 'SQuAD v2.0']\n",
      "['Big Data', 'systems', 'decisions', 'deep learning', 'methods', 'neural networks', 'statistical flexibility', 'computational scalability', 'Bayesian', 'inference']\n",
      "['deep learning', 'Bayesian uncertainty', 'neural networks', 'dropout', 'approximate inference', 'stochastic regularisation', 'model uncertainty', 'active learning', 'image processing', 'reinforcement learning']\n",
      "['Dropout', 'overfitting', 'training', 'performance', 'regularization methods', 'supervised learning tasks', 'benchmark data sets']\n",
      "['tiny colour images', 'unsupervised training', 'deep generative models', 'multi-layer generative model', 'meaningful features', 'human visual cortex', 'parallelization algorithm', 'object recognition', 'CIFAR-10', 'CIFAR-100']\n",
      "['Empirical Bayes Method', 'Symmetric Multivariate', 'Cyclic Designs', 'Statistical Reasoning', 'Generalized Additive Models', 'Contingency Tables', 'Markov Models', 'Predictive Inference', 'Bootstrap']\n",
      "['posterior', 'evidence', 'predictive density', 'linear multivariate regression', 'zero-mean Gaussian noise', 'Bayesian', 'model selection', 'basis function regression']\n",
      "['gradient-based learning', 'back-propagation algorithm', 'handwritten character recognition', 'graph transformer networks', 'global training', 'handwritten digit recognition', 'online handwriting recognition', 'document recognition systems']\n",
      "['bandit processes', 'dynamic allocation indices', 'intractable problems', 'stochastic scheduling', 'sequential clinical trials', 'search problems']\n",
      "已更新 74 个已处理的摘要标题。\n",
      "共现关系已保存到 'data1/1483_cooccurrence.csv'。\n",
      "处理完成。\n",
      "['Intelligence', 'knowledge', 'ensemble-based', 'computational costs', 'epinet', 'neural network', 'pretrained models', 'estimate uncertainty', 'Bayesian neural networks', 'epistemic neural network (ENN)']\n",
      "未找到已处理摘要记录文件，初始化一个新的记录。\n",
      "未找到共现关系文件，初始化一个新的计数器。\n",
      "['Brain-inspired computing', 'artificial intelligence', 'computational neuroscience', 'hardware architectures', 'software tool', 'benchmark data', 'modeling/algorithm', 'interdisciplinary field', 'real-world applications', 'research ecology']\n",
      "['synaptic connectivity', 'neural circuits', 'connectomes', 'probabilistic generative models', 'olfactory bulb of zebrafish', 'mouse visual cortex', 'C. elegans', 'synapses', 'synthetic circuits', 'design principles']\n",
      "['Spiking Neural Networks', 'energy-efficiency', 'back-propagation through time', 'SLAYER', 'neuron reset mechanism', 'numerical instability', 'gradient scale', 'Implicit Function Theorem', 'EXODUS', 'training complexity']\n",
      "['large-scale model', 'area V1', 'visual processing', 'brain-like neural network', 'noise robustness', 'neural coding properties', 'Convolutional Neural Networks', 'energy-efficient', 'neuromorphic hardware', 'visual cortex']\n",
      "['pruning', 'deep neural networks', 'parameter initialization', 'nonzero biases', 'universal approximation property', 'ReLU activation functions', 'orthogonal parameter initialization', 'benchmark data', 'strong lottery ticket pruning']\n",
      "['Brax', 'open source library', 'rigid body simulation', 'performance', 'parallelism', 'JAX', 'accelerators', 'reinforcement learning', 'PPO', 'SAC']\n",
      "['spiking neural networks', 'surrogate gradients', 'connectivity', 'learning performance', 'classification problems', 'activity regularization', 'sparse activity', 'numerical simulations', 'hidden units']\n",
      "['spiking neural networks', 'reinforcement learning', 'central pattern generators', 'energy constraints', 'computationally intensive', 'spike time dependent plasticity', 'training a legged robot', 'synchronization patterns', 'raspberry pi platform', 'integrated sensors']\n",
      "['Energy-efficient mapless navigation', 'mobile robots', 'spiking neural networks', 'spiking deep deterministic policy gradient', 'spiking actor network', 'gradient descent', 'Intel’s Loihi', 'neuromorphic processor', 'brain-inspired algorithms']\n",
      "['lottery ticket hypothesis', 'randomly-initialized network', 'subnetwork', 'performance', 'original network', 'bounded distribution', 'target network', 'bounded weights', 'over-parameterized neural network', 'random weights']\n",
      "['randomly weighted neural networks', 'subnetworks', 'untrained subnetworks', 'Wide ResNet-50', 'ResNet-34', 'ImageNet', 'algorithm', 'fixed weights']\n",
      "['SpiNNaker', 'ARM-based processor', 'spiking neural networks', '1 Million core', '10 Million core', '22nm FDSOI', 'runtime adaptive body biasing', 'numerical accelerators', 'deep neural networks']\n",
      "['neural network architectures', 'weight parameters', 'search method', 'reinforcement learning tasks', 'supervised learning', 'MNIST', 'random weights', 'performance']\n",
      "['biologically realistic model', 'mouse primary visual cortex', 'biophysically-detailed', 'point-neurons', 'network connectivity', 'neural activity', 'visual stimuli', 'cell-class specific connectivity', 'synaptic strengths', 'firing rate distributions']\n",
      "['Lottery Ticket Hypothesis', 'sparse networks', 'initial weights', 'performance', 'Lottery Ticket (LT) algorithm', 'weights to zero', 'signs', 'reinitialized network', 'masking', 'Supermasks']\n",
      "['spiking network model', 'macaque visual cortex', 'microcircuit', 'asynchronous irregular ground state', 'spiking statistics', 'synapses', 'excitatory neurons', 'inhibitory neurons', 'metastable regime']\n",
      "['deep Spiking Neural Networks', 'low power spike event based computation', 'non-differentiable', 'backpropagation', 'synaptic weights', 'axonal delays', 'temporal credit assignment', 'GPU accelerated software', 'state of the art performance']\n",
      "['sparse architectures', 'lottery ticket hypothesis', 'subnetworks', 'winning tickets', 'fortuitous initializations', 'feed-forward networks', 'test accuracy', 'initial weights', 'MNIST and CIFAR10']\n",
      "['Loihi', '60-mm2 chip', '14-nm process', 'spiking neural networks', 'programmable synaptic learning rules', 'spiking convolutional', 'Locally Competitive Algorithm', 'LASSO optimization', 'spike-based computation', 'energy-delay-product']\n",
      "['black-box optimization', 'deep reinforcement learning', 'directed exploration', 'novelty search', 'quality diversity', 'scalability', 'local optima', 'Atari', 'exploring agents']\n",
      "['evolution strategy', 'natural evolution strategy', 'deep reinforcement learning', 'neural network parameters', 'finite-difference approximation', 'reward gradient', 'average reward', 'robust to perturbation', 'policy gradient-based reinforcement learning', 'robustness-seeking']\n",
      "['stochastic gradient descent', 'neural networks', 'reinforcement learning', 'evolution strategy', 'MNIST', 'supervised learning', 'ES population sizes', 'RL', 'SGD-based proxy', 'evolutionary method']\n",
      "['Deep artificial neural networks', 'gradient-based learning algorithms', 'deep reinforcement learning', 'genetic algorithm', 'neural networks', 'neuroevolution', 'novelty search', 'parallelizes', 'compact encoding technique']\n",
      "['policy gradient methods', 'reinforcement learning', 'stochastic gradient ascent', 'proximal policy optimization', 'PPO', 'trust region policy optimization', 'TRPO', 'sample complexity', 'benchmark tasks']\n",
      "['brain-like behaviors', 'supervised training', 'spatio-temporal information', 'backpropagation', 'non-differentiable', 'approximated derivative', 'gradient descent', 'spatial domain', 'temporal domain']\n",
      "['spiking neural networks', 'supervised learning', 'surrogate gradient', 'SuperSpike', 'multilayer networks', 'integrate-and-fire neurons', 'spatiotemporal spike patterns', 'feedback alignment', 'credit assignment', 'symmetric feedback']\n",
      "['Evolution Strategies', 'black box optimization', 'MDP-based RL', 'Q-learning', 'Policy Gradients', 'MuJoCo', 'Atari', 'CPUs', 'communication strategy', 'temporal discounting']\n",
      "['brain-inspired computing', 'deep convolutional neural networks', 'classification tasks', 'neuromorphic hardware', 'energy-efficiency', 'state-of-the-art accuracy', 'spiking neurons', 'recognition tasks', 'backpropagation', 'deep learning']\n",
      "['TrueNorth', 'neurosynaptic processor', 'low-power', 'non-von Neumann', 'event-driven', 'scalable', 'cognitive computing', 'brain-inspired architectures', 'visual object recognition', 'CMOS scaling']\n",
      "['spiking neural networks', 'SNN', 'biologically plausible mechanisms', 'unsupervised learning', 'MNIST benchmark', 'spike-timing-dependent plasticity', 'lateral inhibition', 'adaptive spiking threshold', 'heterogeneous biological neural networks']\n",
      "['neural network model', 'recurrent neural networks', 'sequence of symbols', 'vector representation', 'encoder', 'decoder', 'conditional probability', 'statistical machine translation', 'linguistic phrases']\n",
      "['stochastic neurons', 'hard non-linearities', 'gradient estimator', 'REINFORCE algorithm', 'binary stochastic neuron', 'multiplicative noise', 'straight-through estimator', 'conditional computation']\n",
      "['criticality', 'metastability', 'dynamic range', 'power-law scaling', 'neuronal avalanches', 'oscillations', 'excitation and inhibition', 'homeostatic plasticity', 'critical behavior']\n",
      "['natural evolution strategies', 'multivariate normal distribution', 'self-adapting mutation matrix', 'covariance matrix adaption', 'evolution strategy', 'Monte Carlo estimate', 'natural gradient', 'expected fitness', 'multimodal tasks']\n",
      "['neural modeling', 'recurrent circuits', 'integrate-and-fire neurons', 'real-time computing', 'high-dimensional dynamical systems', 'statistical learning theory', 'universal analog fading memory', 'readout neurons', 'liquid state machine', 'neural coding']\n",
      "['depression', 'glutamatergic synapses', 'correlated spiking', 'long-term potentiation', 'long-term depression', 'NMDA receptors', 'GABAergic', 'L-type calcium channels', 'Hebb’s rule']\n",
      "['long short-term memory', 'LSTM', 'gradient based method', 'time lags', 'constant error flow', 'computational complexity', 'recurrent learning', 'artificial data']\n",
      "['backpropagation', 'artificial neural networks', 'pattern recognition', 'fault diagnosis', 'backpropagation through time', 'dynamic systems', 'systems identification', 'control', 'recurrent networks', 'ordered derivatives']\n",
      "['Deep Reinforcement Learning', 'robotics', 'energy-efficient motions', 'action penalty', 'Spiking Neural Network', 'gait experiments', 'hexapod agent', 'Soft Actor-Critic', 'Twin Delayed Deep Deterministic policy gradient', 'Deep Deterministic policy gradient']\n",
      "['learning algorithm', 'recurrent neural networks', 'weights', 'output units']\n",
      "已更新 40 个已处理的摘要标题。\n",
      "共现关系已保存到 'data1/1484_cooccurrence.csv'。\n",
      "处理完成。\n",
      "['Recurrent spiking neural networks', 'artificial general intelligence', 'surrogate gradient-based training', 'neuromorphic hardware', 'evolving connectivity', 'Natural Evolution Strategies', 'hardware-friendly', 'sparse boolean connections', 'robotic locomotion tasks', 'neuromorphic devices.']\n",
      "未找到已处理摘要记录文件，初始化一个新的记录。\n",
      "未找到共现关系文件，初始化一个新的计数器。\n",
      "['retinal encoding', 'natural movies', 'behaviorally-relevant features', 'compressed latent space', 'encoder-decoder', 'retinal ganglion cells', 'low-dimensional representation', 'static textures', 'velocity features', 'time in the natural scene']\n",
      "['Bit Diffusion', 'discrete data', 'analog bits', 'sample quality', 'discrete image generation', 'CIFAR-10', 'ImageNet-64x64', 'FID', 'image captioning']\n",
      "['neurons', 'complex dynamics', 'GLIFR model', 'rate-based neuronal model', 'after-spike currents', 'machine learning', 'gradient descent', 'temporal tasks', 'sequential MNIST', 'robustness']\n",
      "['neural population activity', 'behaviour', 'neural variability', 'Targeted Neural Dynamical Modeling', 'state-space model', 'neural activity', 'variational autoencoder', 'latent dynamics']\n",
      "['spiking neural networks', 'temporal coding', 'rank coding', 'artificial neural networks', 'LSTMs', 'computational savings', 'speedups', 'threshold-crossing event', 'sequence classification', 'MNIST dataset']\n",
      "['Spiking neural networks', 'neuromorphic devices', 'image generation', 'variational autoencoder', 'latent space', 'autoregressive SNN model', 'Bernoulli process', 'variational learning', 'Fully Spiking Variational Autoencoder', 'datasets']\n",
      "['neural activity', 'unsupervised approach', 'disentangled representations', 'Swap-VAE', 'generative modeling framework', 'instance-specific alignment loss', 'brain state', 'temporal consistency', 'neural datasets', 'latent dimensions']\n",
      "['probabilistic recurrent spiking network', 'likelihood', 'neural activity', 'log-likelihood', 'dissimilarity', 'summary statistics', 'back-propagation', 'hidden neurons', 'network connectivity', 'spike recordings']\n",
      "['cascaded diffusion models', 'high fidelity images', 'class-conditional ImageNet', 'super-resolution diffusion models', 'conditioning augmentation', 'FID scores', 'classification accuracy scores']\n",
      "['SR3', 'Super-Resolution', 'Repeated Refinement', 'denoising diffusion probabilistic models', 'U-Net architecture', 'magnification factors', 'CelebA-HQ', 'fool rate', 'ImageNet', 'ResNet-50 classifier']\n",
      "['DDPM', 'generative models', 'log-likelihoods', 'sample quality', 'sampling', 'reverse diffusion process', 'precision and recall', 'GANs', 'target distribution', 'model capacity']\n",
      "['neural population responses', 'deep generative models', 'identifiable', 'latent variable models', 'task variables', 'pi-VAE', 'rat hippocampus', 'macaque motor cortex', 'neural codes']\n",
      "['autoregressive spike train model', 'maximum likelihood estimation', 'likelihood', 'spike train kernels', 'neural recorded', 'model generated spike trains', 'maximum mean discrepancy', 'neural data', 'spike train kernels', 'model-mismatch']\n",
      "['surrogate gradients', 'spiking networks', 'learning performance', 'non-differentiable nonlinearity', 'activity regularization', 'connectivity', 'classification problems', 'sparse activity limit']\n",
      "['generalized linear models', 'neuronal networks', 'complex stimuli', 'coupling parameters', 'blowup instabilities', 'two-step inference strategy', 'retinal ganglion cells', 'visual stimuli', 'interaction networks', 'deep convolutional neural networks']\n",
      "['Spiking neural networks', 'event-driven neuromorphic processors', 'error backpropagation', 'spiking discontinuities', 'temporal learning', 'Temporal Spike Sequence Learning Backpropagation', 'inter-neuron dependencies', 'intra-neuron dependencies', 'temporal window', 'image classification datasets']\n",
      "['limited capacity', 'recent memory', 'forgetting', 'lossy compression models', 'Information Bottleneck', 'P300', 'novelty-detection', 'auditory oddball', 'memory characteristics', 'surprise responses']\n",
      "['SNNs', 'neuromorphic spiking NN processors', 'synaptic plasticity', 'data-driven learning']\n",
      "['natural video', 'visual cortex', 'deep recurrent network', 'neural activity', 'V1 neurons', 'two-photon microscopy', 'domain transfer', 'orientation tuning', 'natural images', 'artificial noise stimuli']\n",
      "['Generative Adversarial Networks', 'Wasserstein-GAN', 'neural population activity', 'Spike-GAN', 'spike trains', 'salamander retina', 'maximum entropy', 'dichotomized Gaussian', 'importance maps', 'systems neuroscience']\n",
      "['efficient coding', 'sensory neurons', 'neural responses', 'predictive coding', 'past inputs', 'future prediction', 'neural circuits', 'diversity of neural responses', 'stimulus statistics', 'visual motion tuning']\n",
      "['brain-like behaviors', 'spatio-temporal information', 'pre-training', 'backpropagation', 'performance bottleneck', 'non-differentiable', 'spatio-temporal backpropagation', 'approximated derivative', 'gradient descent training']\n",
      "['spiking neural networks', 'supervised learning', 'surrogate gradient', 'SuperSpike', 'voltage-based three-factor learning rule', 'integrate-and-fire neurons', 'spatiotemporal spike patterns', 'feedback alignment', 'credit assignment', 'nonlinear computations']\n",
      "['deep convolutional neural networks', 'retinal responses', 'natural scenes', 'Generalized Linear Models', 'feature maps', 'feedforward inhibition', 'sub-Poisson spiking variability', 'recurrent lateral connections', 'contrast adaptation']\n",
      "['multitask recurrent neural network', 'neurons', 'predictive models', 'primate retina', 'generalized linear models', 'spiking responses', 'multilayer recurrent neural networks', 'parasol ganglion cells', 'natural images', 'GLM-RNN hybrid model']\n",
      "[]\n",
      "['neural population codes', 'tensor factorizations', 'spatial firing patterns', 'temporal firing patterns', 'trial-dependent activation coefficients', 'sensory stimuli', 'spike timing', 'first-spike latencies', 'retinal ganglion cells', 'natural images']\n",
      "['gradient-free optimization', 'multimodal functions', 'warm restarts', 'stochastic gradient descent', 'deep neural networks', 'CIFAR-10', 'CIFAR-100', 'EEG recordings', 'ImageNet dataset']\n",
      "['information bottleneck', 'IB method', 'code', 'information', 'original data', 'high-dimensional', 'non-gaussian', 'variational scheme', 'lower bound', 'algorithm']\n",
      "['linear filtering', 'generalized linear model', 'retinal ganglion cells', 'naturalistic visual stimuli', 'primate RGCs', 'ON and OFF parasol', 'macaque', 'multi-electrode recordings', 'spatial nonlinearities', 'gain control']\n",
      "['neural network modelling', 'computer vision', 'artificial intelligence applications', 'primate visual hierarchy', 'internal representations', 'representational spaces', 'neurobiologically faithful', 'high-level feats of intelligence']\n",
      "['latent random variables', 'recurrent neural network', 'variational autoencoder', 'variational RNN', 'natural speech', 'sequential data', 'speech datasets', 'handwriting dataset']\n",
      "['Deep Recurrent Attentive Writer', 'neural network architecture', 'image generation', 'spatial attention mechanism', 'foveation', 'variational auto-encoding', 'iterative construction', 'generative models', 'MNIST', 'Street View House Numbers dataset']\n",
      "['RNNs', 'SGVB', 'Variational Recurrent Auto-Encoder', 'VRAE', 'unsupervised learning', 'time series data', 'latent vector representation', 'generative', 'unlabeled data', 'supervised training']\n",
      "['variational inference', 'recurrent neural networks', 'latent variables', 'Stochastic Recurrent Networks', 'stochastic gradient methods', 'multi-modal conditionals', 'marginal likelihood', 'deterministic recurrent neural networks', 'polyphonic musical data sets']\n",
      "['theoretical neuroscience', 'Hodgkin-Huxley equations', 'Hopfield model', 'Generalized Linear Models', 'decision theory', 'neural code', 'differential equations', 'probabilities', 'textbook']\n",
      "['noise', 'computing systems', 'nanoscale devices', 'spiking neurons', 'probabilistic inference', 'sampling', 'creative problem solving', 'self-organization', 'learning from rewards', 'neuromorphic networks']\n",
      "['positive-definite kernels', 'spike trains', 'Hilbert space', 'computational neuroscientists', 'signal processing experts', 'kernel methods', 'point processes', 'positive definite functions']\n",
      "['SPIKE-distance', 'spike train synchrony', 'time-resolved', 'eventlike firing patterns', 'clustering', 'triggered temporal averaging', 'causal SPIKE-distance', 'epileptic seizure', 'electroencephalographic (EEG)']\n",
      "['spike train observations', 'Euclidean structure', 'positive-definite kernels', 'structural representation', 'statistical measures', 'point processes', 'hypothesis testing', 'probability law', 'divergence', 'kernel principal component analysis']\n",
      "['modeling symbolic sequences', 'polyphonic music', 'piano-roll representation', 'probabilistic model', 'distribution estimators', 'recurrent neural network', 'temporal dependencies', 'high-dimensional sequences', 'musical language model', 'symbolic prior']\n",
      "['correlated binary spike trains', 'latent multivariate gaussian model', 'spike trains', 'neural systems', 'simulation', 'neural spike counts', 'pairwise correlations', 'entropy', 'temporal structure', 'populations of neurons']\n",
      "['spiking neurons', 'sensory inputs', 'spike generation', 'retinal ganglion cell', 'leaky integrate-and-fire', 'Gaussian noise', 'likelihood', 'stochastic visual stimulus', 'sensory stimulus selectivity', 'decoding rule']\n",
      "['distance between two spike trains', 'time constant', 'coincidence detector', 'rate difference counter', 'intrinsic noise']\n",
      "['white noise technique', 'spiking visual system neurons', 'response properties', 'simultaneous recordings', 'classical linear systems analysis', 'elementary linear algebra', 'statistics', 'retinal ganglion cells', 'classical approaches']\n",
      "['response reliability', 'lateral geniculate nucleus', 'visual stimuli', 'firing rate', 'temporal precision', 'firing probability', 'temporal patterns', 'redundancy']\n",
      "['information', 'signal', 'prediction', 'code', 'bottleneck', 'rate distortion theory', 'distortion measure', 'coding rules', 'self consistent equations', 'Blahut-Arimoto algorithm']\n",
      "['integrate-and-fire neurons', 'noise', 'time-dependent input', 'diffusion approximation', 'membrane potential', 'escape noise', 'hazard function', 'voltage', 'periodic input']\n",
      "['coincidence detection', 'integrate-and-fire neuron', 'mean output firing rate', 'neuronal dynamics', 'input statistics', 'postsynaptic response function', 'synapses', 'optimal threshold', 'temporal code', 'rate code']\n",
      "['metrics', 'neuronal impulse trains', 'temporal coding', 'point-process', 'firing pattern', 'stimulus-dependent', 'topological structures', 'multidimensional scaling', 'spike times', 'spike intervals']\n",
      "['noisy spiking neurons', 'boolean circuits', 'finite automata', 'shunting inhibition', 'unreliable spiking neurons', 'McCulloch-Pitts neuron', 'threshold gate', 'multilayer perceptron', 'biological neural systems']\n",
      "['information theory', 'redundancy reduction', 'noise', 'optimal encoding', 'parvocellular retinal processing', 'nondivergent transformations', 'signal to noise', 'transfer functions']\n",
      "[]\n",
      "['natural stimuli', 'neuronal spikes', 'visual system', 'Information Bottleneck', 'Gaussian Process', 'spike responses', 'ex-vivo retinas', 'latent variables', 'perception', 'computation']\n",
      "['importance sampling', 'posterior distribution', 'variational distribution', 'evidence upper bounds', 'Auto-Encoding Variational Bayes', 'ELBO', 'VAE', 'MNIST', 'single-cell RNA sequencing', 'multiple hypothesis testing']\n",
      "['variational approximation', 'information bottleneck', 'Tishby et al. (1999)', 'neural network', 'reparameterization trick', 'efficient training', 'Deep Variational Information Bottleneck', 'VIB objective', 'generalization performance', 'robustness to adversarial attack']\n",
      "['Non‐Gaussian processes', 'Ornstein–Uhlenbeck', 'distributional deviations', 'dependence structures', 'statistical analysis', 'finance', 'econometrics', 'continuous time stochastic volatility models', 'financial assets']\n",
      "['spike trains', 'synchrony', 'total correlation', 'early sensory systems', 'signal correlation', 'noise correlation', 'stimulus', 'trial-to-trial', 'sensory stimulus']\n",
      "已更新 58 个已处理的摘要标题。\n",
      "共现关系已保存到 'data1/1485_cooccurrence.csv'。\n",
      "处理完成。\n",
      "['neural response', 'sensory processing', 'neural computations', 'temporal dependencies', 'spiking neurons', 'spike trains', 'temporal conditioning', 'neural coding systems', 'predictive computational accounts', 'sensory perception circuits']\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"\\n# 需要读取解析的时候使用eval函数\\nfrom collections import Counter\\ndf = pd.read_excel('data/all_focus.xlsx')\\n# 确保所有值都是字符串（跳过 NaN 或非字符串）\\ndef safe_eval(cell):\\n    if isinstance(cell, str):\\n        return eval(cell, {'Counter': Counter})\\n    return cell\\n# 使用 eval() 解析字符串恢复为 Counter\\ndf['co_occurrence_counter'] = df['co_occurrence_counter'].apply(safe_eval)\\n# 打印解析后的结果\\nprint(df['co_occurrence_counter'][0])  # 输出: Counter({'This': 1, 'is': 1, 'an': 1, 'example': 1, 'abstract.': 1})\\n\""
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "处理遗漏的单篇论文方法"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:07:30.541526Z",
     "start_time": "2025-01-17T02:07:26.692451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_excel('data1/all_A_papers.xlsx')\n",
    "abstract = \"\"\"\n",
    "Although image restoration has achieved significant progress, its potential to assist object detectors in adverse imaging conditions lacks enough attention. It is reported that the existing image restoration methods cannot improve the object detector performance and sometimes even reduce the detection performance. To address the issue, we propose a targeted adversarial attack in the restoration procedure to boost object detection performance after restoration. Specifically, we present an ADAM-like adversarial attack to generate pseudo ground truth for restoration training. Resultant restored images are close to original sharp images, and at the same time, lead to better results of object detection. We conduct extensive experiments in image dehazing and low light enhancement and show the superiority of our method over conventional training and other domain adaptation and multi-task methods. The proposed pipeline can be applied to all restoration methods and detectors in both one-and two-stage.\n",
    "\"\"\"\n",
    "co_occurrence_counter = liucheng(abstract)\n",
    "df.loc[1445,'co_occurrence_counter'] = str(co_occurrence_counter)\n",
    "df.to_excel('data1/all_A_papers.xlsx',index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image restoration', 'object detectors', 'adversarial attack', 'restoration procedure', 'ADAM-like', 'pseudo ground truth', 'image dehazing', 'low light enhancement', 'domain adaptation', 'multi-task methods']\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "need_process = [16, 35, 40, 50, 53, 56, 66, 88, 133, 149, 184, 254, 260, 279, 366, 631, 751, 785, 890, 918, 1082, 1118, 1123, 1217, 1235, 1277, 1324]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751\n",
      "未找到已处理摘要记录文件，初始化一个新的记录。\n",
      "未找到共现关系文件，初始化一个新的计数器。\n",
      "['model evaluation', 'data-oriented', 'representation-oriented', 'process-oriented evaluation', 'rule induction', 'error estimation']\n",
      "['average case analysis', 'induction algorithms', 'monotone M of N', 'Boolean attributes', 'class noise', 'computational tractability', 'normal distribution', 'supervised induction', 'conditional probability distribution']\n",
      "['representation-oriented', 'process-oriented evaluation', 'generalization error', 'search process', 'rule induction', 'CN2 rule learner', 'high-dimensional domains', 'simpler models']\n",
      "['Model selection', 'Bayesian', 'priors on error values', 'expected true error', 'classification learning', 'empirical minimizer', 'error distribution', 'PAC theory', 'VC-dimension', '10-fold cross-validation']\n",
      "['machine learning algorithms', 'benchmark data sets', 'domain-specific parameters', 'error rate', 'optimistic bias', 'more parameters', 'fewer parameters', 'C4.5', 'FOIL', 'unbiased ranking experiments']\n",
      "['complexity-penalization', 'hold-out error estimation', 'intrinsic metric structure', 'hypothesis space', 'unlabeled training patterns', 'empirical error estimates', 'overfitting', 'polynomial curve fitting', 'function learning tasks']\n",
      "['test error', 'voting classification rule', 'margin', 'training examples', 'support vector classifiers', 'neural networks', 'test error', 'bias-variance']\n",
      "['bias/variance decomposition', 'model selection', 'hypothesis classes', 'penalty terms', 'model complexity', 'under-fitting', 'over-fitting', 'variance profile', 'complexity-penalization', 'regression problems']\n",
      "['statistical learning', 'text categorization', 'dimensionality reduction', 'document frequency', 'information gain', 'mutual information', 'term strength']\n",
      "['cross-validation data', 'cross-validation error', 'hypotheses', 'randomized learning algorithm', 'noise', 'LOOCVCV', 'experimental results', 'cross-validation sets']\n",
      "['quadratic loss functions', 'learning algorithms', 'negative variance', 'frequency based estimation']\n",
      "['AdaBoost', 'pseudo-loss', 'learning algorithm', 'multi-label concepts', 'real learning problems', 'decision trees', 'machine-learning benchmarks', 'nearest-neighbor classifier']\n",
      "['cross validation', 'approximation rate', 'estimation rate', 'hypothesis model', 'overfitting', 'generalization errors', 'sample size', 'model selection problem']\n",
      "['nonparametric regression', 'error backpropagation', 'statistical viewpoint', 'recognition experiments', 'artificial data', 'handwritten numerals', 'machine perception', 'machine learning', 'neural modeling']\n",
      "['knowledge acquisition', 'explicit programming', 'computational viewpoint', 'information gathering mechanism', 'learning protocol', 'algorithmic complexity', 'realistic learning systems']\n",
      "['true error rate', 'cross validation', 'model selection algorithm', 'generalization error', 'empirical error', 'overfitting', 'Occam algorithms', 'AdaBoost algorithm']\n",
      "['boosting', 'predictive power', 'decision trees', 'weights of training instances']\n",
      "['GML', 'generalized spline smoothing', 'bias plus variance', 'overfitting avoidance', 'learning curve', 'Bayes optimal classification', 'PAC model']\n",
      "['reproduction of a training set', 'generalization error', 'hypothesis function', 'non-Euclidean inner product', 'Bayesian formalism', 'target function', 'training set', 'over-training', 'cross-validation', 'stacked generalization']\n",
      "['Probably Approximately Correct (PAC) learning', 'target function', 'agnostic learning', 'hardness results', 'dynamic programming', 'loss functions', 'hidden variables']\n",
      "['neural network', 'unimportant weights', 'generalization', 'training examples', 'speed of learning', 'classification', 'second-derivative', 'network complexity', 'training set error']\n",
      "['mean squared error', 'd X', 'm', 'n', 'π(x)']\n",
      "已更新 22 个已处理的摘要标题。\n",
      "共现关系已保存到 'data1/751_cooccurrence.csv'。\n",
      "处理完成。\n",
      "['generalization performance', 'training error', 'distribution of error rates', 'model selection algorithm', 'learning curves', 'Boolean decision trees', 'text categorization']\n"
     ]
    }
   ],
   "execution_count": 12,
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('data1/all_A_papers.xlsx')\n",
    "import time\n",
    "# 找到df中所有co_occurrence_counter为空的，如果为空就进行处理\n",
    "for index in need_process:\n",
    "    print(index)\n",
    "    try:\n",
    "        get_kw_(ref_file=f'data1/{index}_ref.csv', cooccurrence_csv = f'data1/{index}_cooccurrence.csv',processed_csv= f'data1/{index}_processed.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        continue\n",
    "    if pd.notna(df.loc[index,'co_occurrence_counter']):\n",
    "        continue\n",
    "    else:\n",
    "        df1 = pd.read_csv(f'data1/{index}_paper.csv')\n",
    "        abstract = df1['abstract'][0]\n",
    "        if pd.isna(abstract) or not abstract:\n",
    "            continue\n",
    "        co_occurrence_counter = liucheng(abstract)\n",
    "        df.loc[index,'co_occurrence_counter'] = str(co_occurrence_counter)\n",
    "        df.to_excel('data1/all_A_papers.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "breakthrough",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
