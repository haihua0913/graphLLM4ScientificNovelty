node1,node2,similarity,distance
ping-pong-playing robot,reward function,-0.0447,1.4455
ping-pong-playing robot,policy parameters,-0.1448,1.5132
exploration vs. exploitation,ping-pong-playing robot,0.0497,1.3786
cumulative reward,ping-pong-playing robot,-0.1433,1.5121
policy parameters,reward function,0.972,0.2367
exploration vs. exploitation,reward function,-0.4092,1.6788
cumulative reward,reward function,0.9475,0.3241
exploration vs. exploitation,policy parameters,-0.3094,1.6183
cumulative reward,policy parameters,0.9475,0.3239
cumulative reward,exploration vs. exploitation,-0.4375,1.6956
Gaussian Process Bandit Optimization,reward function,-0.3156,1.6221
exploration-bias advice,reward function,-0.1411,1.5107
Gaussian Process Bandit Optimization,policy parameters,-0.4519,1.704
exploration-bias advice,policy parameters,-0.2354,1.5718
Gaussian Process Bandit Optimization,exploration vs. exploitation,-0.0302,1.4354
exploration vs. exploitation,exploration-bias advice,0.1142,1.331
Gaussian Process Bandit Optimization,cumulative reward,-0.3423,1.6385
cumulative reward,exploration-bias advice,-0.2542,1.5838
Gaussian Process Bandit Optimization,exploration-bias advice,-0.2182,1.5609
